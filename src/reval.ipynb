{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from model_input_old import input_pipeline\n",
    "from model import CNNModel, RNNModel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {}\n",
    "# Get from dataset.\n",
    "config['num_test_samples'] = 2174\n",
    "config['lr'] = {}\n",
    "config['lr']['batch_size'] = 16\n",
    "config['batch_size'] = 16\n",
    "\n",
    "config['num_epochs'] = 1\n",
    "config['model_dir'] = '../runs/lr=(False_0.97_exponential_1000_0.001_16)_cnn=(0.5_(5x3x3x3)_(16x32x64x128)_(512x256xNone))_rnn=(20_average_1_256_16)_1497524781'\n",
    "config['checkpoint_id'] = None # If None, the last checkpoint will be used.\n",
    "\n",
    "config['img_height'] = 80\n",
    "config['img_width'] = 80\n",
    "config['img_num_channels'] = 3\n",
    "config['skeleton_size'] = 180\n",
    "\n",
    "# CNN model parameters\n",
    "config['cnn'] = pickle.load(open(os.path.join(config['model_dir'], \"config_cnn.pkl\"), 'rb'))\n",
    "\n",
    "# RNN model parameters\n",
    "config['rnn'] = pickle.load(open(os.path.join(config['model_dir'], \"config_rnn.pkl\"), 'rb'))\n",
    "\n",
    "config['ip_queue_capacity'] = config['batch_size']*50\n",
    "config['ip_num_read_threads'] = 1\n",
    "\n",
    "config['test_data_dir'] = \"../data/test/\"\n",
    "config['test_file_format'] = \"dataTest_%d.tfrecords\"\n",
    "config['test_file_ids'] = list(range(1,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating /home/martipa/project/runs/lr=(False_0.97_exponential_1000_0.001_16)_cnn=(0.5_(5x3x3x3)_(16x32x64x128)_(512x256xNone))_rnn=(20_average_1_256_16)_1497524781/model-3570\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tfRecord input files.\n",
    "test_filenames = [os.path.join(config['test_data_dir'], config['test_file_format'] % i) for i in config['test_file_ids']]\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "test_batch_samples_op, test_batch_ids_op, test_batch_seq_len_op = input_pipeline(test_filenames, config, name='test_input_pipeline', shuffle=False, mode=\"inference\")\n",
    "\n",
    "sess = tf.Session()\n",
    "init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "# Test graph.\n",
    "with tf.name_scope(\"Inference\"):\n",
    "    # Create model\n",
    "    inferCnnModel = CNNModel(config=config['cnn'],\n",
    "                            input_op=test_batch_samples_op, \n",
    "                            mode='inference')\n",
    "    \n",
    "    infer_cnn_representations = inferCnnModel.build_graph()\n",
    "    \n",
    "    inferModel = RNNModel(config=config['rnn'], \n",
    "                            input_op=infer_cnn_representations, \n",
    "                            target_op=None, \n",
    "                            seq_len_op=test_batch_seq_len_op,\n",
    "                          \n",
    "                            mode=\"inference\")\n",
    "    inferModel.build_graph()\n",
    "    \n",
    "# Restore computation graph.\n",
    "saver = tf.train.Saver()\n",
    "# Restore variables.\n",
    "checkpoint_path = config['checkpoint_id']\n",
    "if checkpoint_path is None:\n",
    "    checkpoint_path = tf.train.latest_checkpoint(config['model_dir'])\n",
    "print(\"Evaluating \" + checkpoint_path)\n",
    "saver.restore(sess, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Create a list of tfRecord input files.\n",
    "# test_filenames = [os.path.join(config['test_data_dir'], config['test_file_format'] % i) for i in config['test_file_ids']]\n",
    "# # Create data loading operators. This will be represented as a node in the computational graph.\n",
    "# test_batch_samples_op, test_batch_ids_op, test_batch_seq_len_op = input_pipeline(test_filenames, config, name='test_input_pipeline', shuffle=False, mode=\"inference\")\n",
    "\n",
    "# sess = tf.Session()\n",
    "# init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "# sess.run(init_op)\n",
    "\n",
    "# coord = tf.train.Coordinator()\n",
    "# threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "# # Test graph.\n",
    "# with tf.name_scope(\"Inference\"):\n",
    "#     # Create model\n",
    "#     inferCnnModel = CNNModel(config=config['cnn'],\n",
    "#                             input_op=test_batch_samples_op, \n",
    "#                             mode='inference')\n",
    "    \n",
    "#     infer_cnn_representations = inferCnnModel.build_graph()\n",
    "    \n",
    "#     inferModel = RNNModel(config=config['rnn'], \n",
    "#                             input_op=infer_cnn_representations, \n",
    "#                             target_op=None, \n",
    "#                             seq_len_op=test_batch_seq_len_op,\n",
    "#                             mode=\"inference\")\n",
    "    \n",
    "#     inferModel.build_graph()\n",
    "    \n",
    "# # Restore computation graph.\n",
    "# saver = tf.train.Saver()\n",
    "\n",
    "# # restore variables\n",
    "\n",
    "# if config['checkpoint_id'] is None:\n",
    "#     checkpoint_path = tf.train.latest_checkpoint(config['model_dir'])\n",
    "# else:\n",
    "#     checkpoint_path = os.path.abspath(os.path.join(config['model_dir'], config['checkpoint_id']))\n",
    "    \n",
    "# print(\"Evaluating \" + checkpoint_path)\n",
    "# saver.restore(sess, checkpoint_path)\n",
    "\n",
    "# print('\\ndone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop\n",
    "test_predictions = []\n",
    "test_sample_ids = []\n",
    "try:\n",
    "    while not coord.should_stop():\n",
    "        # Get predicted labels and sample ids for submission csv.\n",
    "        [predictions, sample_ids] = sess.run([inferModel.predictions, test_batch_ids_op], feed_dict={})\n",
    "        test_predictions.extend(predictions)\n",
    "        test_sample_ids.extend(sample_ids)\n",
    "\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Done.')\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()   \n",
    "\n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)\n",
    "\n",
    "# Now you have your predictions. Do whatever you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(test_predictions[:100])\n",
    "print(test_sample_ids[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2174, 2)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "final_predictions = [x+1 for x in test_predictions]\n",
    "\n",
    "submission = np.zeros((config['num_test_samples'], 2))\n",
    "\n",
    "for i in range(0, config['num_test_samples']):\n",
    "    submission[i, 0] = i + 1\n",
    "    submission[i, 1] = final_predictions[i]\n",
    "    \n",
    "print(submission.shape)\n",
    "    \n",
    "# should add checkpoint to name\n",
    "submission_name = '../submissions/submission_{}.csv'.format(\"1\")\n",
    "\n",
    "with open(submission_name, 'wb') as f:\n",
    "    f.write(b\"Id,Prediction\\n\")\n",
    "    np.savetxt(f,submission,fmt='%i', delimiter=',')\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test_predictions = np.zeros((0,))\n",
    "# test_sample_ids = np.zeros((0,))\n",
    "\n",
    "# try:\n",
    "#     while not coord.should_stop():\n",
    "#         # Get predicted labels and sample ids for submission csv.\n",
    "#         [predictions, sample_ids] = sess.run([inferModel.predictions, test_batch_ids_op], feed_dict={})\n",
    "#         test_predictions = np.concatenate((test_predictions, predictions))\n",
    "#         test_sample_ids = np.concatenate((test_sample_ids, sample_ids))\n",
    "\n",
    "# except tf.errors.OutOfRangeError:\n",
    "#     print('done')\n",
    "# finally:\n",
    "#     # When done, ask the threads to stop.\n",
    "#     coord.request_stop()   \n",
    "\n",
    "# # Wait for threads to finish.\n",
    "# coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions += 1\n",
    "submission = np.stack((test_prediction, test_sample_ids), axis=1)\n",
    "\n",
    "# unclean, checkpoint_id possibly None, but oh lord...\n",
    "submission_name = '../submission_{}'.format(config['model_dir'])\n",
    "\n",
    "with open(submission_name, 'wb') as f:\n",
    "    f.write(b\"Id,Prediction\\n\")\n",
    "    np.savetxt(f,submission,fmt='%i', delimiter=',')\n",
    "    \n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

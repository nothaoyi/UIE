{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from model_input import input_pipeline\n",
    "from model import CNNModel, RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to:\n",
      "/home/martipa/project/runs/lr=(1000_False_0.001_exponential_0.97)_cnn=((8x16x32x64)_(256xNonexNone)_0.5_(5x3x3x3))_rnn=(3_average_128_20_16)_1497551213\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "# Get from dataset.\n",
    "config['num_test_samples'] = 2174\n",
    "config['num_validation_samples'] = 1765\n",
    "config['num_training_samples'] = 5722\n",
    "\n",
    "config['batch_size'] = 16\n",
    "\n",
    "# epoch, checkpoints, validation, prints, log dir\n",
    "config['num_steps_per_epoch'] = int(config['num_training_samples']/config['batch_size'])\n",
    "\n",
    "config['num_epochs'] = 1000\n",
    "config['evaluate_every_step'] = config['num_steps_per_epoch']*3\n",
    "config['checkpoint_every_step'] = config['num_steps_per_epoch']\n",
    "config['num_validation_steps'] = int(config['num_validation_samples']/config['batch_size'])\n",
    "config['print_every_step'] = config['num_steps_per_epoch']//5\n",
    "config['log_dir'] = '../runs/'\n",
    "\n",
    "# input params\n",
    "config['img_height'] = 80\n",
    "config['img_width'] = 80\n",
    "config['img_num_channels'] = 3\n",
    "config['skeleton_size'] = 180\n",
    "\n",
    "# learning rate params\n",
    "config['lr'] = {}\n",
    "config['lr']['value'] = 1e-3\n",
    "# config['lr']['type'] = 'fixed'\n",
    "config['lr']['type'] = 'exponential'\n",
    "config['lr']['decay_steps'] = 1000\n",
    "config['lr']['decay_rate'] = 0.97\n",
    "config['lr']['staircase'] = False\n",
    "\n",
    "# CNN model params\n",
    "config['cnn'] = {}\n",
    "config['cnn']['kernels'] = [5,3,3,3]\n",
    "config['cnn']['num_filters'] = [8,16,32,64] # Number of filters for every convolutional layer.\n",
    "config['cnn']['num_hidden_units'] = [256,None,None] # Number of output units, i.e. representation size.\n",
    "config['cnn']['dropout_rate'] = 0.5\n",
    "\n",
    "# RNN model params\n",
    "config['rnn'] = {}\n",
    "config['rnn']['num_hidden_units'] = 128 # Number of units in an LSTM cell.\n",
    "config['rnn']['num_layers'] = 3 # Number of LSTM stack.\n",
    "config['rnn']['num_class_labels'] = 20\n",
    "config['rnn']['batch_size'] = config['batch_size']\n",
    "config['rnn']['loss_type'] = 'average' # or 'last_step' # In the case of 'average', average of all time-steps is used instead of the last time-step.\n",
    "\n",
    "# reader config\n",
    "config['ip_queue_capacity'] = config['batch_size']*50\n",
    "config['ip_num_read_threads'] = 6\n",
    "\n",
    "# data paths and formats\n",
    "config['train_data_dir'] = \"../data/train/\"\n",
    "config['train_file_format'] = \"dataTrain_%d.tfrecords\"\n",
    "config['train_file_ids'] = list(range(1,41))\n",
    "config['valid_data_dir'] = \"../data/validation/\"\n",
    "config['valid_file_format'] = \"dataValidation_%d.tfrecords\"\n",
    "config['valid_file_ids'] = list(range(1,16))\n",
    "\n",
    "\n",
    "# Create a unique output directory for this experiment.\n",
    "timestamp = str(int(time.time()))\n",
    "\n",
    "model_name = ''\n",
    "\n",
    "for c in ['lr', 'cnn', 'rnn']:\n",
    "    model_name += c+'=('\n",
    "    for (_,value) in config[c].items():\n",
    "        model_name += str(value)+'_'\n",
    "    model_name += ')_'\n",
    "    \n",
    "model_name += timestamp\n",
    "\n",
    "model_name = model_name.replace('_)', ')')\n",
    "model_name = model_name.replace(', ', 'x')\n",
    "model_name = model_name.replace('[', '(')\n",
    "model_name = model_name.replace(']', ')')\n",
    "\n",
    "config['model_dir'] = os.path.abspath(os.path.join(config['log_dir'], model_name))\n",
    "print(\"Saving model to:\\n{}\".format(config['model_dir']))\n",
    "\n",
    "print('\\ndone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of parameters: 897636\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tfRecord input files.\n",
    "train_filenames = [os.path.join(config['train_data_dir'], config['train_file_format'] % i) for i in config['train_file_ids']]\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "train_batch_samples_op, train_batch_labels_op, train_batch_seq_len_op = input_pipeline(train_filenames, config, name='training_input_pipeline')\n",
    "\n",
    "# Create a list of tfRecord input files.\n",
    "valid_filenames = [os.path.join(config['valid_data_dir'], config['valid_file_format'] % i) for i in config['valid_file_ids']]\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "valid_batch_samples_op, valid_batch_labels_op, valid_batch_seq_len_op = input_pipeline(valid_filenames, config, name='validation_input_pipeline', mode='validation')\n",
    "\n",
    "# Create placeholders for training and monitoring variables.\n",
    "loss_avg_op = tf.placeholder(tf.float32, name=\"loss_avg\")\n",
    "accuracy_avg_op = tf.placeholder(tf.float32, name=\"accuracy_avg\")\n",
    "\n",
    "# Generate a variable to contain a counter for the global training step.\n",
    "# Note that it is useful if you save/restore your network.\n",
    "global_step = tf.Variable(1, name='global_step', trainable=False)\n",
    "\n",
    "# Create seperate graphs for training and validation.\n",
    "# Training graph\n",
    "# Note that our model is optimized by using the training graph.\n",
    "with tf.name_scope(\"Training\"):\n",
    "    # Create model\n",
    "    cnnModel = CNNModel(config=config['cnn'],\n",
    "                        input_op=train_batch_samples_op, \n",
    "                        mode='training')\n",
    "    cnn_representations = cnnModel.build_graph()\n",
    "    \n",
    "    trainModel = RNNModel(config=config['rnn'], \n",
    "                            input_op=cnn_representations, \n",
    "                            target_op=train_batch_labels_op, \n",
    "                            seq_len_op=train_batch_seq_len_op,\n",
    "                            mode=\"training\")\n",
    "    trainModel.build_graph()\n",
    "    print(\"# of parameters: %s\\n\" % trainModel.num_parameters)\n",
    "    \n",
    "    # Optimization routine.\n",
    "    # Learning rate is decayed in time. This enables our model using higher learning rates in the beginning.\n",
    "    # In time the learning rate is decayed so that gradients don't explode and training staurates.\n",
    "    # If you observe slow training, feel free to modify decay_steps and decay_rate arguments.\n",
    "    if config['lr']['type'] == 'exponential':\n",
    "        learning_rate = tf.train.exponential_decay(config['lr']['value'], \n",
    "                                                   global_step=global_step,\n",
    "                                                   decay_steps=config['lr']['decay_steps'], \n",
    "                                                   decay_rate=config['lr']['decay_rate'],\n",
    "                                                   staircase=config['lr']['staircase'])\n",
    "    elif config['lr']['type'] == 'fixed':\n",
    "        learning_rate = config['lr']['value']\n",
    "    else:\n",
    "        print(\"Invalid learning rate type\")\n",
    "        raise\n",
    "        \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(trainModel.loss, global_step=global_step)\n",
    "\n",
    "# Validation graph.\n",
    "with tf.name_scope(\"Evaluation\"):\n",
    "    # Create model\n",
    "    validCnnModel = CNNModel(config=config['cnn'],\n",
    "                                input_op=valid_batch_samples_op, \n",
    "                                mode='validation')\n",
    "    valid_cnn_representations = validCnnModel.build_graph()\n",
    "    \n",
    "    validModel = RNNModel(config=config['rnn'], \n",
    "                            input_op=valid_cnn_representations, \n",
    "                            target_op=valid_batch_labels_op, \n",
    "                            seq_len_op=valid_batch_seq_len_op,\n",
    "                            mode=\"validation\")\n",
    "    validModel.build_graph()\n",
    "\n",
    "    \n",
    "# Create summary ops for monitoring the training.\n",
    "# Each summary op annotates a node in the computational graph and collects\n",
    "# data data from it.\n",
    "summary_train_loss = tf.summary.scalar('loss', trainModel.loss)\n",
    "summary_train_acc = tf.summary.scalar('accuracy_training', trainModel.batch_accuracy)\n",
    "summary_avg_accuracy = tf.summary.scalar('accuracy_avg', accuracy_avg_op)\n",
    "summary_avg_loss = tf.summary.scalar('loss_avg', loss_avg_op)\n",
    "summary_learning_rate = tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "# Group summaries.\n",
    "# summaries_training is used during training and reported after every step.\n",
    "summaries_training = tf.summary.merge([summary_train_loss, summary_train_acc, summary_learning_rate])\n",
    "# summaries_evaluation is used by both trainig and validation in order to report the performance on the dataset.\n",
    "summaries_evaluation = tf.summary.merge([summary_avg_accuracy, summary_avg_loss])\n",
    "    \n",
    "#Create session object\n",
    "sess = tf.Session()\n",
    "# Add the ops to initialize variables.\n",
    "init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "# Actually intialize the variables\n",
    "sess.run(init_op)\n",
    "\n",
    "# Register summary ops.\n",
    "train_summary_dir = os.path.join(config['model_dir'], \"summary\", \"train\")\n",
    "train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "valid_summary_dir = os.path.join(config['model_dir'], \"summary\", \"validation\")\n",
    "valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "\n",
    "# Create a saver for writing training checkpoints.\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "# Define counters in order to accumulate measurements.\n",
    "counter_correct_predictions_training = 0.0\n",
    "counter_loss_training = 0.0\n",
    "counter_correct_predictions_validation = 0.0\n",
    "counter_loss_validation = 0.0\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "print(\"Saving config files to:\\n{}\".format(config['model_dir']))\n",
    "pickle.dump(config['cnn'], open(os.path.join(config['model_dir'], \"config_cnn.pkl\"), \"wb\"))\n",
    "pickle.dump(config['rnn'], open(os.path.join(config['model_dir'], \"config_rnn.pkl\"), \"wb\"))\n",
    "pickle.dump(config['lr'],  open(os.path.join(config['model_dir'], \"config_lr.pkl\"),  \"wb\"))\n",
    "\n",
    "print('\\ndone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training Loop\n",
    "try:\n",
    "    while not coord.should_stop():\n",
    "        step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "        if (step%config['checkpoint_every_step']) == 0:\n",
    "            ckpt_save_path = saver.save(sess, os.path.join(config['model_dir'], 'model'), global_step)\n",
    "            print(\"Model saved in file: %s\" % ckpt_save_path)\n",
    "            \n",
    "        # Run the optimizer to update weights.\n",
    "        # Note that \"train_op\" is responsible from updating network weights.\n",
    "        # Only the operations that are fed are evaluated.\n",
    "        # Run the optimizer to update weights.\n",
    "        train_summary, num_correct_predictions, loss, _ = sess.run([summaries_training, \n",
    "                                                                      trainModel.num_correct_predictions, \n",
    "                                                                      trainModel.loss, \n",
    "                                                                      train_op], \n",
    "                                                                      feed_dict={})\n",
    "        # Update counters.\n",
    "        counter_correct_predictions_training += num_correct_predictions\n",
    "        counter_loss_training += loss\n",
    "        # Write summary data.\n",
    "        train_summary_writer.add_summary(train_summary, step)\n",
    "        \n",
    "        # Report training performance\n",
    "        if (step%config['print_every_step']) == 0:\n",
    "            accuracy_avg = counter_correct_predictions_training / (config['lr']['batch_size']*config['print_every_step'])\n",
    "            loss_avg = counter_loss_training / (config['print_every_step'])\n",
    "            summary_report = sess.run(summaries_evaluation, feed_dict={accuracy_avg_op:accuracy_avg, loss_avg_op:loss_avg})\n",
    "            train_summary_writer.add_summary(summary_report, step)\n",
    "            print(\"[%d/%d] [Training] Accuracy: %.3f, Loss: %.3f\" % (step/config['num_steps_per_epoch'], \n",
    "                                                                     step, \n",
    "                                                                     accuracy_avg, \n",
    "                                                                     loss_avg))\n",
    "            \n",
    "            counter_correct_predictions_training = 0.0\n",
    "            counter_loss_training= 0.0\n",
    "        \n",
    "        if (step%config['evaluate_every_step']) == 0:\n",
    "            # It is possible to create only one input pipelene queue. Hence, we create a validation queue \n",
    "            # in the begining for multiple epochs and control it via a foor loop.\n",
    "            # Note that we only approximate 1 validation epoch (validation doesn't have to be accurate.)\n",
    "            # In other words, number of unique validation samples may differ everytime.\n",
    "            for eval_step in range(config['num_validation_steps']):\n",
    "                # Calculate average validation accuracy.\n",
    "                num_correct_predictions, loss = sess.run([validModel.num_correct_predictions, \n",
    "                                                          validModel.loss],\n",
    "                                                         feed_dict={})\n",
    "                # Update counters.\n",
    "                counter_correct_predictions_validation += num_correct_predictions\n",
    "                counter_loss_validation += loss\n",
    "            \n",
    "            # Report validation performance\n",
    "            accuracy_avg = counter_correct_predictions_validation / (config['lr']['batch_size']*config['num_validation_steps'])\n",
    "            loss_avg = counter_loss_validation / (config['num_validation_steps'])\n",
    "            summary_report = sess.run(summaries_evaluation, feed_dict={accuracy_avg_op:accuracy_avg, loss_avg_op:loss_avg})\n",
    "            valid_summary_writer.add_summary(summary_report, step)\n",
    "            print(\"[%d/%d] [Validation] Accuracy: %.3f, Loss: %.3f\" % (step/config['num_steps_per_epoch'], \n",
    "                                                                       step, \n",
    "                                                                       accuracy_avg, \n",
    "                                                                       loss_avg))\n",
    "            \n",
    "            counter_correct_predictions_validation = 0.0\n",
    "            counter_loss_validation= 0.0\n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Model is trained for %d epochs, %d steps.' % (config['num_epochs'], step))\n",
    "    print('Done.')\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()\n",
    "\n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)\n",
    "\n",
    "ckpt_save_path = saver.save(sess, os.path.join(config['model_dir'], 'model'), global_step)\n",
    "print(\"Model saved in file: %s\" % ckpt_save_path)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def read_and_decode_sequence(filename_queue, config):\n",
    "#     # Create a TFRecordReader.\n",
    "#     readerOptions = tf.python_io.TFRecordOptions(compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
    "#     reader = tf.TFRecordReader(options=readerOptions)\n",
    "#     _, serialized_example = reader.read(filename_queue)\n",
    "    \n",
    "#     # Read one sequence sample.\n",
    "#     # The training and validation files contains the following fields:\n",
    "#     # - label: label of the sequence which take values between 1 and 20.\n",
    "#     # - length: length of the sequence, i.e., number of frames.\n",
    "#     # - depth: sequence of depth images. [length x height x width x numChannels]\n",
    "#     # - rgb: sequence of rgb images. [length x height x width x numChannels]\n",
    "#     # - segmentation: sequence of segmentation maskes. [length x height x width x numChannels]\n",
    "#     # - skeleton: sequence of flattened skeleton joint positions. [length x numJoints]\n",
    "#     #\n",
    "#     # The test files doesn't contain \"label\" field.\n",
    "#     # [height, width, numChannels] = [80, 80, 3]\n",
    "#     with tf.name_scope(\"TFRecordDecoding\"):\n",
    "#         context_encoded, sequence_encoded = tf.parse_single_sequence_example(\n",
    "#                 serialized_example,\n",
    "#                 # \"label\" and \"lenght\" are encoded as context features. \n",
    "#                 context_features={\n",
    "#                     \"label\": tf.FixedLenFeature([], dtype=tf.int64),\n",
    "#                     \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "#                 },\n",
    "#                 # \"depth\", \"rgb\", \"segmentation\", \"skeleton\" are encoded as sequence features.\n",
    "#                 sequence_features={\n",
    "#                     \"depth\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "#                     \"rgb\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "#                     \"segmentation\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "#                     \"skeleton\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "#                 })\n",
    "\n",
    "        \n",
    "#         # Fetch required data fields. \n",
    "#         # TODO: Customize for your design. Assume that only the RGB images are used for now.\n",
    "#         # Decode the serialized RGB images.    \n",
    "#         seq_rgb = tf.decode_raw(sequence_encoded['rgb'], tf.uint8)\n",
    "#         seq_seg = tf.decode_raw(sequence_encoded['segmentation'], tf.uint8)\n",
    "#         seq_dep = tf.decode_raw(sequence_encoded['depth'], tf.uint8)\n",
    "        \n",
    "#         # Output dimensionality: [seq_len, height, width, numChannels]\n",
    "        \n",
    "#         # tf.map_fn applies the preprocessing function to every image in the sequence, i.e., frame.\n",
    "#         seq_rgb = tf.map_fn(lambda x: preprocessing_op(x, config),\n",
    "#                                 elems=(seq_rgb,seq_seg,seq_dep),\n",
    "#                                 dtype=tf.float32,\n",
    "#                                 back_prop=False)\n",
    "        \n",
    "#         seq_label = context_encoded['label']\n",
    "        \n",
    "#         # Tensorflow requires the labels start from 0. Before you create submission csv, \n",
    "#         # increment the predictions by 1.\n",
    "#         seq_label = seq_label - 1\n",
    "#         seq_len = tf.to_int32(context_encoded['length'])\n",
    "        \n",
    "#         \"\"\"\n",
    "#         # Use skeleton only.\n",
    "#         seq_skeleton = tf.decode_raw(sequence_encoded['skeleton'], tf.float32)\n",
    "#         # Normalize skeleton so that every pose is a unit length vector.\n",
    "#         seq_skeleton = tf.nn.l2_normalize(seq_skeleton, dim=1)\n",
    "#         seq_skeleton.set_shape([None, config['skeleton_size']])\n",
    "        \n",
    "#         seq_len = tf.to_int32(context_encoded['length'])\n",
    "#         seq_label = context_encoded['label']\n",
    "#         # Tensorflow requires the labels start from 0. Before you create submission csv, \n",
    "#         # increment the predictions by 1.\n",
    "#         seq_label = seq_label - 1\n",
    "#         \"\"\"\n",
    "        \n",
    "#         return [seq_rgb, seq_label, seq_len]\n",
    "    \n",
    "\n",
    "# def input_pipeline(filenames, config, name='input_pipeline', shuffle=True):\n",
    "#     with tf.name_scope(name):\n",
    "#         # Create a queue of TFRecord input files.\n",
    "#         filename_queue = tf.train.string_input_producer(filenames, num_epochs=config['num_epochs'], shuffle=shuffle)\n",
    "#         # Read the data from TFRecord files, decode and create a list of data samples by using threads.\n",
    "#         sample_list = [read_and_decode_sequence(filename_queue, config) for _ in range(config['ip_num_read_threads'])]\n",
    "#         # Create batches.\n",
    "#         # Since the data consists of variable-length sequences, allow padding by setting dynamic_pad parameter.\n",
    "#         # \"batch_join\" creates batches of samples and pads the sequences w.r.t the max-length sequence in the batch.\n",
    "#         # Hence, the padded sequence length can be different for different batches.\n",
    "#         batch_rgb, batch_labels, batch_lens = tf.train.batch_join(sample_list,\n",
    "#                                                     batch_size=config['batch_size'],\n",
    "#                                                     capacity=config['ip_queue_capacity'],\n",
    "#                                                     enqueue_many=False,\n",
    "#                                                     dynamic_pad=True,\n",
    "#                                                     allow_smaller_final_batch = False,\n",
    "#                                                     name=\"batch_join_and_pad\")\n",
    "\n",
    "#         return batch_rgb, batch_labels, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def preprocessing_op(frame, config):\n",
    "#     \"\"\"\n",
    "#     Creates preprocessing operations that are going to be applied on a single frame.\n",
    "    \n",
    "#     TODO: Customize for your needs.\n",
    "#     You can do any preprocessing (masking, normalization/scaling of inputs, augmentation, etc.) by using tensorflow operations.\n",
    "#     Built-in image operations: https://www.tensorflow.org/api_docs/python/tf/image \n",
    "#     \"\"\"\n",
    "#     with tf.name_scope(\"preprocessing\"):\n",
    "#         rgb = frame[0]\n",
    "#         seg = frame[1]\n",
    "#         dep = frame[2]\n",
    "        \n",
    "#         # Reshape serialized image.\n",
    "#         rgb = tf.reshape(rgb, (config['img_height'], \n",
    "#                                config['img_width'], \n",
    "#                                config['img_num_channels'])\n",
    "#                         )\n",
    "        \n",
    "#         seg = tf.reshape(seg, (config['img_height'], \n",
    "#                                config['img_width'], \n",
    "#                                config['img_num_channels'])\n",
    "#                         )\n",
    "        \n",
    "#         dep = tf.reshape(dep, (config['img_height'], \n",
    "#                                config['img_width'], \n",
    "#                                1)\n",
    "#                         )\n",
    "        \n",
    "#         # Convert from RGB to grayscale.\n",
    "#         rgb = tf.image.rgb_to_grayscale(rgb)\n",
    "#         seg = tf.image.rgb_to_grayscale(seg)\n",
    "#         dep = tf.image.rgb_to_grayscale(dep)\n",
    "        \n",
    "#         # Integer to float.\n",
    "#         rgb = tf.to_float(rgb)\n",
    "#         dep = tf.to_float(dep)\n",
    "        \n",
    "#         seg = tf.greater(seg, 150)\n",
    "        \n",
    "#         rgb = tf.where(seg, rgb, tf.zeros_like(rgb))\n",
    "#         dep = tf.where(seg, dep, tf.zeros_like(rgb))\n",
    "        \n",
    "#         # Crop\n",
    "#         #image_op = tf.image.resize_image_with_crop_or_pad(image_op, 60, 60)\n",
    "        \n",
    "#         # Resize operation requires 4D tensors (i.e., batch of images).\n",
    "#         # Reshape the image so that it looks like a batch of one sample: [1,60,60,1]\n",
    "#         #image_op = tf.expand_dims(image_op, 0)\n",
    "#         # Resize\n",
    "#         #image_op = tf.image.resize_bilinear(image_op, np.asarray([32,32]))\n",
    "#         # Reshape the image: [32,32,1]\n",
    "#         #image_op = tf.squeeze(image_op, 0)\n",
    "        \n",
    "#         # Normalize (zero-mean unit-variance) the image locally, i.e., by using statistics of the \n",
    "#         # image not the whole data or sequence.\n",
    "        \n",
    "#         rgb = tf.image.per_image_standardization(rgb)\n",
    "#         dep = tf.image.per_image_standardization(dep)\n",
    "        \n",
    "#         rgb = tf.squeeze(rgb)\n",
    "#         dep = tf.squeeze(dep)\n",
    "        \n",
    "#         # Flatten image\n",
    "#         #image_op = tf.reshape(image_op, [-1])\n",
    "    \n",
    "#         return tf.stack([rgb, dep], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class CNNModel():\n",
    "#     \"\"\"\n",
    "#     Creates training and validation computational graphs.\n",
    "#     Note that tf.variable_scope enables sharing the parameters so that both graphs share the parameters.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, config, input_op, mode):\n",
    "#         \"\"\"\n",
    "#         Basic setup.\n",
    "#         Args:\n",
    "#           config: Object containing configuration parameters.\n",
    "#         \"\"\"\n",
    "#         assert mode in [\"training\", \"validation\"]\n",
    "#         self.config = config\n",
    "#         self.inputs = input_op\n",
    "#         self.mode = mode\n",
    "#         self.is_training = self.mode == \"training\"\n",
    "#         self.reuse = self.mode == \"validation\"\n",
    "        \n",
    "        \n",
    "#     def build_model(self, input_layer):\n",
    "#         with tf.variable_scope(\"cnn_model\", reuse=self.reuse, initializer=tf.contrib.layers.xavier_initializer()):\n",
    "#             # Convolutional Layer #1\n",
    "#             # Computes 32 features using a 3x3 filter with ReLU activation.\n",
    "#             # Padding is added to preserve width and height.\n",
    "#             # Input Tensor Shape: [batch_size, 80, 80, num_channels]\n",
    "#             # Output Tensor Shape: [batch_size, 40, 40, num_filter1]\n",
    "#             conv1 = tf.layers.conv2d(\n",
    "#                 inputs=input_layer,\n",
    "#                 filters=self.config['cnn_filters'][0],\n",
    "#                 kernel_size=[3, 3],\n",
    "#                 padding=\"same\",\n",
    "#                 activation=tf.nn.relu)\n",
    "\n",
    "#             pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2, padding='same')\n",
    "\n",
    "#             # Input Tensor Shape: [batch_size, 40, 40, num_filter1]\n",
    "#             # Output Tensor Shape: [batch_size, 20, 20, num_filter2]\n",
    "#             conv2 = tf.layers.conv2d(\n",
    "#                 inputs=pool1,\n",
    "#                 filters=self.config['cnn_filters'][1],\n",
    "#                 kernel_size=[5, 5],\n",
    "#                 padding=\"same\",\n",
    "#                 activation=tf.nn.relu)\n",
    "\n",
    "#             pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2, padding='same')\n",
    "\n",
    "#             # Input Tensor Shape: [batch_size, 20, 20, num_filter2]\n",
    "#             # Output Tensor Shape: [batch_size, 10, 10, num_filter3]\n",
    "#             conv3 = tf.layers.conv2d(\n",
    "#                 inputs=pool2,\n",
    "#                 filters=self.config['cnn_filters'][2],\n",
    "#                 kernel_size=[3, 3],\n",
    "#                 padding=\"same\",\n",
    "#                 activation=tf.nn.relu)\n",
    "\n",
    "#             pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2, padding='same')\n",
    "\n",
    "#             # Input Tensor Shape: [batch_size, 10, 10, num_filter3]\n",
    "#             # Output Tensor Shape: [batch_size, 5, 5, num_filter4]\n",
    "#             conv4 = tf.layers.conv2d(\n",
    "#                 inputs=pool3,\n",
    "#                 filters=self.config['cnn_filters'][3],\n",
    "#                 kernel_size=[3, 3],\n",
    "#                 padding=\"same\",\n",
    "#                 activation=tf.nn.relu)\n",
    "\n",
    "#             pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2, padding='same')\n",
    "\n",
    "\n",
    "#             # Flatten tensor into a batch of vectors\n",
    "#             # Input Tensor Shape: [batch_size, 5, 5, num_filter4]\n",
    "#             # Output Tensor Shape: [batch_size, 5 * 5 * num_filter4]\n",
    "#             conv_flat = tf.reshape(pool4, [-1, 5 * 5 * self.config['cnn_filters'][3]])\n",
    "\n",
    "#             # Add dropout operation;\n",
    "#             dropout = tf.layers.dropout(inputs=conv_flat, rate=self.config['dropout_rate'], training=self.is_training)\n",
    "\n",
    "#             # Dense Layer\n",
    "#             # Densely connected layer with <num_hidden_units> neurons\n",
    "#             # Input Tensor Shape: [batch_size, 5 * 5 * num_filter4]\n",
    "#             # Output Tensor Shape: [batch_size, num_hidden_units]\n",
    "#             dense = tf.layers.dense(inputs=dropout, units=self.config['num_hidden_units'],\n",
    "#                 activation=tf.nn.relu)\n",
    "        \n",
    "#             self.cnn_model = dense\n",
    "#             return dense\n",
    "            \n",
    "#     def build_graph(self):\n",
    "#         \"\"\"\n",
    "#         CNNs accept inputs of shape (batch_size, height, width, num_channels). However, we have inputs of shape\n",
    "#         (batch_size, sequence_length, height, width, num_channels) where sequence_length is inferred at run time.\n",
    "#         We need to iterate in order to get CNN representations. Similar to python's map function, \"tf.map_fn\" \n",
    "#         applies a given function on each entry in the input list. \n",
    "#         \"\"\"\n",
    "#         # For the first time create a dummy graph and then share the parameters everytime.\n",
    "#         if self.is_training:\n",
    "#             self.reuse = False\n",
    "#             self.build_model(self.inputs[0])\n",
    "#             self.reuse = True\n",
    "        \n",
    "#         # CNN takes a clip as if it is a batch of samples.\n",
    "#         # Have a look at tf.map_fn (https://www.tensorflow.org/api_docs/python/tf/map_fn)\n",
    "#         # You can set parallel_iterations or swap_memory in order to make it faster.\n",
    "#         # Note that back_prop argument is True in order to enable training of CNN.\n",
    "#         self.cnn_representations = tf.map_fn(lambda x: self.build_model(x),\n",
    "#                                                 elems=self.inputs,\n",
    "#                                                 dtype=tf.float32,\n",
    "#                                                 back_prop=True,\n",
    "#                                                 swap_memory=True,\n",
    "#                                                 parallel_iterations=2)\n",
    "        \n",
    "#         return self.cnn_representations\n",
    "        \n",
    "\n",
    "# class RNNModel():\n",
    "#     \"\"\"\n",
    "#     Creates training and validation computational graphs.\n",
    "#     Note that tf.variable_scope enables sharing the parameters so that both graphs share the parameters.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, config, input_op, target_op, seq_len_op, mode):\n",
    "#         \"\"\"\n",
    "#         Basic setup.\n",
    "#         Args:\n",
    "#           config: Object containing configuration parameters.\n",
    "#         \"\"\"\n",
    "#         assert mode in [\"training\", \"validation\"]\n",
    "#         self.config = config\n",
    "#         self.inputs = input_op\n",
    "#         self.targets = target_op\n",
    "#         self.seq_lengths = seq_len_op\n",
    "#         self.mode = mode\n",
    "#         self.reuse = self.mode == \"validation\"\n",
    "        \n",
    "#     def build_rnn_model(self):\n",
    "#         with tf.variable_scope('rnn_cell', reuse=self.reuse, initializer=tf.contrib.layers.xavier_initializer()):\n",
    "#             rnn_cell = tf.contrib.rnn.BasicLSTMCell(num_units=self.config['num_hidden_units'])\n",
    "#         with tf.variable_scope('rnn_stack', reuse=self.reuse, initializer=tf.contrib.layers.xavier_initializer()):\n",
    "#             if self.config['num_layers'] > 1:\n",
    "#                 rnn_cell = tf.contrib.rnn.MultiRNNCell([rnn_cell for _ in range(self.config['num_layers'])])\n",
    "#             self.model_rnn, self.rnn_state = tf.nn.dynamic_rnn(\n",
    "#                                             cell=rnn_cell,\n",
    "#                                             inputs=self.inputs,\n",
    "#                                             dtype = tf.float32,\n",
    "#                                             sequence_length=self.seq_lengths,\n",
    "#                                             time_major=False,\n",
    "#                                             swap_memory=True)\n",
    "#             # Fetch output of the last step.\n",
    "#             if self.config['loss_type'] == 'last_step':\n",
    "#                 self.rnn_prediction = tf.gather_nd(self.model_rnn, tf.stack([tf.range(self.config['batch_size']), self.seq_lengths-1], axis=1))\n",
    "#             elif self.config['loss_type'] == 'average':\n",
    "#                 self.rnn_prediction = self.model_rnn\n",
    "#             else:\n",
    "#                 print(\"Invalid loss type\")\n",
    "#                 raise\n",
    "                \n",
    "    \n",
    "#     def build_model(self):\n",
    "#         self.build_rnn_model()\n",
    "#         # Calculate logits\n",
    "#         with tf.variable_scope('logits', reuse=self.reuse, initializer=tf.contrib.layers.xavier_initializer()):\n",
    "#             self.logits = tf.layers.dense(inputs=self.rnn_prediction, units=self.config['num_class_labels'],\n",
    "#                                             kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "#                                             bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "#             # In the case of average loss, take average of time steps in order to calculate\n",
    "#             # final prediction probabilities.\n",
    "#             if self.config['loss_type'] == 'average':\n",
    "#                 self.logits = tf.reduce_mean(self.logits, axis=1)\n",
    "            \n",
    "#     def loss(self):\n",
    "#         # Loss calculations: cross-entropy\n",
    "#         with tf.name_scope(\"cross_entropy_loss\"):\n",
    "#             self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.targets))\n",
    "            \n",
    "#                 # Accuracy calculations.\n",
    "#         with tf.name_scope(\"accuracy\"):\n",
    "#             # Return list of predictions (useful for making a submission)\n",
    "#             self.predictions = tf.argmax(self.logits, 1, name=\"predictions\")\n",
    "#             # Return a bool tensor with shape [batch_size] that is true for the\n",
    "#             # correct predictions.\n",
    "#             self.correct_predictions = tf.equal(tf.argmax(self.logits, 1), self.targets)\n",
    "#             # Number of correct predictions in order to calculate average accuracy afterwards.\n",
    "#             self.num_correct_predictions = tf.reduce_sum(tf.cast(self.correct_predictions, tf.int32))\n",
    "#             # Calculate the accuracy per minibatch.\n",
    "#             self.batch_accuracy = tf.reduce_mean(tf.cast(self.correct_predictions, tf.float32))\n",
    "        \n",
    "#     def build_graph(self):\n",
    "#         self.build_model()\n",
    "#         self.loss()\n",
    "#         self.num_parameters()\n",
    "        \n",
    "#         return self.logits, self.loss, self.batch_accuracy, self.predictions\n",
    "    \n",
    "#     def num_parameters(self):\n",
    "#         self.num_parameters = 0\n",
    "#         #iterating over all variables\n",
    "#         for variable in tf.trainable_variables():\n",
    "#             local_parameters=1\n",
    "#             shape = variable.get_shape()  #getting shape of a variable\n",
    "#             for i in shape:\n",
    "#                 local_parameters*=i.value  #mutiplying dimension values\n",
    "#             self.num_parameters+=local_parameters\n",
    "# print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

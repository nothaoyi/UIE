{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "An example of TFRecord data loading, preprocessing and RNN training.\n",
    "    - Creates threads to read TFRecord files from disk, decode and preprocess.\n",
    "    - Crops and resizes the RGB frames, i.e., images, (32x32) and flatten: 1024 dimensional representation vector.\n",
    "    - Builds recurrent 2-layer LSTM model\n",
    "    - Trains the model on flattened image vectors.\n",
    "\n",
    "You can use 2D CNN for representation learning on images or 3D volumetric CNN on multiple frames. You should find out how to stack CNN and RNN networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_op(image_op, config):\n",
    "    \"\"\"\n",
    "    Creates preprocessing operations that are going to be applied on a single frame.\n",
    "    \n",
    "    TODO: Customize for your needs.\n",
    "    You can do any preprocessing (masking, normalization/scaling of inputs, augmentation, etc.) by using tensorflow operations.\n",
    "    Built-in image operations: https://www.tensorflow.org/api_docs/python/tf/image \n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"preprocessing\"):\n",
    "        # Reshape serialized image.\n",
    "        image_op = tf.reshape(image_op, (config['img_height'], \n",
    "                                         config['img_width'], \n",
    "                                         config['img_num_channels'])\n",
    "                          )\n",
    "        # Convert from RGB to grayscale.\n",
    "        image_op = tf.image.rgb_to_grayscale(image_op)\n",
    "        \n",
    "        # Integer to float.\n",
    "        image_op = tf.to_float(image_op)\n",
    "        # Crop\n",
    "        image_op = tf.image.resize_image_with_crop_or_pad(image_op, 60, 60)\n",
    "        \n",
    "        # Resize operation requires 4D tensors (i.e., batch of images).\n",
    "        # Reshape the image so that it looks like a batch of one sample: [1,60,60,1]\n",
    "        image_op = tf.expand_dims(image_op, 0)\n",
    "        # Resize\n",
    "        image_op = tf.image.resize_bilinear(image_op, np.asarray([32,32]))\n",
    "        # Reshape the image: [32,32,1]\n",
    "        image_op = tf.squeeze(image_op, 0)\n",
    "        \n",
    "        # Normalize (zero-mean unit-variance) the image locally, i.e., by using statistics of the \n",
    "        # image not the whole data or sequence. \n",
    "        image_op = tf.image.per_image_standardization(image_op)\n",
    "        \n",
    "        # Flatten image\n",
    "        image_op = tf.reshape(image_op, [-1])\n",
    "    \n",
    "        return image_op\n",
    "\n",
    "def read_and_decode_sequence(filename_queue, config):\n",
    "    # Create a TFRecordReader.\n",
    "    readerOptions = tf.python_io.TFRecordOptions(compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    reader = tf.TFRecordReader(options=readerOptions)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    \n",
    "    # Read one sequence sample.\n",
    "    # The training and validation files contains the following fields:\n",
    "    # - label: label of the sequence which take values between 1 and 20.\n",
    "    # - length: length of the sequence, i.e., number of frames.\n",
    "    # - depth: sequence of depth images. [length x height x width x numChannels]\n",
    "    # - rgb: sequence of rgb images. [length x height x width x numChannels]\n",
    "    # - segmentation: sequence of segmentation maskes. [length x height x width x numChannels]\n",
    "    # - skeleton: sequence of flattened skeleton joint positions. [length x numJoints]\n",
    "    #\n",
    "    # The test files doesn't contain \"label\" field.\n",
    "    # [height, width, numChannels] = [80, 80, 3]\n",
    "    with tf.name_scope(\"TFRecordDecoding\"):\n",
    "        context_encoded, sequence_encoded = tf.parse_single_sequence_example(\n",
    "                serialized_example,\n",
    "                # \"label\" and \"lenght\" are encoded as context features. \n",
    "                context_features={\n",
    "                    \"label\": tf.FixedLenFeature([], dtype=tf.int64),\n",
    "                    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "                },\n",
    "                # \"depth\", \"rgb\", \"segmentation\", \"skeleton\" are encoded as sequence features.\n",
    "                sequence_features={\n",
    "                    \"depth\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"rgb\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"segmentation\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"skeleton\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                })\n",
    "\n",
    "        # Fetch required data fields. \n",
    "        # TODO: Customize for your design. Assume that only the RGB images are used for now.\n",
    "        # Decode the serialized RGB images.    \n",
    "        seq_rgb = tf.decode_raw(sequence_encoded['rgb'], tf.uint8)\n",
    "        seq_label = context_encoded['label']\n",
    "        # Tensorflow requires the labels start from 0. Before you create submission csv, \n",
    "        # increment the predictions by 1.\n",
    "        seq_label = seq_label - 1\n",
    "        seq_len = tf.to_int32(context_encoded['length'])\n",
    "        # Output dimnesionality: [seq_len, height, width, numChannels]\n",
    "        # tf.map_fn applies the preprocessing function on every image in the sequence, i.e., frame.\n",
    "        seq_rgb = tf.map_fn(lambda x: preprocessing_op(x, config),\n",
    "                                elems=seq_rgb,\n",
    "                                dtype=tf.float32,\n",
    "                                back_prop=False)\n",
    "    \n",
    "        return [seq_rgb, seq_label, seq_len]\n",
    "    \n",
    "\n",
    "def input_pipeline(filenames, config):\n",
    "    with tf.name_scope(\"input_pipeline\"):\n",
    "        # Create a queue of TFRecord input files.\n",
    "        filename_queue = tf.train.string_input_producer(filenames, num_epochs=config['num_epochs'], shuffle=True)\n",
    "        # Read the data from TFRecord files, decode and create a list of data samples by using threads.\n",
    "        sample_list = [read_and_decode_sequence(filename_queue, config) for _ in range(config['ip_num_read_threads'])]\n",
    "        # Create batches.\n",
    "        # Since the data consists of variable-length sequences, allow padding by setting dynamic_pad parameter.\n",
    "        # \"batch_join\" creates batches of samples and pads the sequences w.r.t the max-length sequence in the batch.\n",
    "        # Hence, the padded sequence length can be different for different batches.\n",
    "        batch_rgb, batch_labels, batch_lens = tf.train.batch_join(sample_list,\n",
    "                                                    batch_size=config['batch_size'],\n",
    "                                                    capacity=config['ip_queue_capacity'],\n",
    "                                                    enqueue_many=False,\n",
    "                                                    dynamic_pad=True,\n",
    "                                                    name=\"batch_join_and_pad\")\n",
    "\n",
    "        return batch_rgb, batch_labels, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /media/eaksan/Warehouse-SSD/Workspace/uie_ss17/assignment3/public/runs/1494252247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "config['checkpoint_every_step'] = 1000\n",
    "config['print_every_step'] = 100\n",
    "config['log_dir'] = './runs/'\n",
    "\n",
    "config['img_height'] = 80\n",
    "config['img_width'] = 80\n",
    "config['img_num_channels'] = 3\n",
    "\n",
    "config['num_epochs'] = 100\n",
    "config['batch_size'] = 32\n",
    "config['learning_rate'] = 1e-3\n",
    "\n",
    "config['num_rnn_hidden_units'] = 512\n",
    "config['num_rnn_layers'] = 2\n",
    "config['num_class_labels'] = 20\n",
    "\n",
    "config['ip_queue_capacity'] = config['batch_size']*50\n",
    "config['ip_num_read_threads'] = 10\n",
    "\n",
    "config['data_dir'] = \"/home/eaksan/uie_data/train/\"\n",
    "config['file_format'] = \"dataTrain_%d.tfrecords\"\n",
    "config['file_ids'] = list(range(1,41))\n",
    "\n",
    "# Create a unique output directory for this experiment.\n",
    "timestamp = str(int(time.time()))\n",
    "config['model_dir'] = os.path.abspath(os.path.join(config['log_dir'], timestamp))\n",
    "print(\"Writing to {}\\n\".format(config['model_dir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SequenceClassifier():\n",
    "    def __init__(self, config, input_op, target_op, seq_len_op):\n",
    "        \"\"\"Basic setup.\n",
    "        Args:\n",
    "          config: Object containing configuration parameters.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.inputs = input_op\n",
    "        self.targets = target_op\n",
    "        self.seq_lengths = seq_len_op\n",
    "        \n",
    "    def build_rnn_model(self):\n",
    "        with tf.variable_scope('rnn_cell'):\n",
    "            rnn_cell = tf.contrib.rnn.BasicLSTMCell(num_units=self.config['num_rnn_hidden_units'])\n",
    "        with tf.variable_scope('rnn_stack'):\n",
    "            if self.config['num_rnn_layers'] > 1:\n",
    "                rnn_cell = tf.contrib.rnn.MultiRNNCell([rnn_cell for _ in range(self.config['num_rnn_layers'])])\n",
    "            self.model_rnn, self.rnn_state = tf.nn.dynamic_rnn(\n",
    "                                            cell=rnn_cell,\n",
    "                                            inputs=self.inputs,\n",
    "                                            dtype = tf.float32,\n",
    "                                            sequence_length=self.seq_lengths)\n",
    "            # Fetch output of the last step.\n",
    "            self.rnn_last_step = tf.gather_nd(self.model_rnn, tf.stack([tf.range(self.config['batch_size']), self.seq_lengths-1], axis=1))\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.build_rnn_model()\n",
    "        # Calculate logits\n",
    "        with tf.variable_scope('logits'):\n",
    "            self.logits = tf.layers.dense(inputs=self.rnn_last_step, units=self.config['num_class_labels'],\n",
    "                                            kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                            bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "    def loss(self):\n",
    "        # Loss calculations: cross-entropy\n",
    "        with tf.name_scope(\"cross_entropy_loss\"):\n",
    "            self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.targets))\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self.build_model()\n",
    "        self.loss()\n",
    "        \n",
    "        return self.logits, self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a list of tfRecord input files.\n",
    "filenames = [os.path.join(config['data_dir'], config['file_format'] % i) for i in config['file_ids']]\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "batch_samples_op, batch_labels_op, batch_seq_len_op = input_pipeline(filenames, config)\n",
    "\n",
    "# Create placeholders for training and monitoring variables.\n",
    "#mode_op = tf.placeholder(tf.bool, name=\"mode\")\n",
    "loss_avg = tf.placeholder(tf.float32, name=\"loss_avg\")\n",
    "accuracy_avg = tf.placeholder(tf.float32, name=\"accuracy_avg\")\n",
    "\n",
    "# Create model\n",
    "seqClassifierModel = SequenceClassifier(config, batch_samples_op, batch_labels_op, batch_seq_len_op)\n",
    "logits, loss = seqClassifierModel.build_graph()\n",
    "\n",
    "# Accuracy calculations.\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    # Return list of predictions (useful for making a submission)\n",
    "    predictions = tf.argmax(logits, 1, name=\"predictions\")\n",
    "    # Return a bool tensor with shape [batch_size] that is true for the\n",
    "    # correct predictions.\n",
    "    #correct_predictions = tf.nn.in_top_k(logits, batch_labels_op, 1)\n",
    "    correct_predictions = tf.equal(tf.argmax(logits, 1), batch_labels_op)\n",
    "    # Calculate the accuracy per minibatch.\n",
    "    batch_accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "    # Number of correct predictions in order to calculate average accuracy afterwards.\n",
    "    num_correct_predictions = tf.reduce_sum(tf.cast(correct_predictions, tf.int32))\n",
    "    \n",
    "# Create summary ops for monitoring the training.\n",
    "# Each summary op annotates a node in the computational graph and collects\n",
    "# data data from it.\n",
    "summary_trian_loss = tf.summary.scalar('loss', loss)\n",
    "summary_train_acc = tf.summary.scalar('accuracy_training', batch_accuracy)\n",
    "summary_avg_accuracy = tf.summary.scalar('accuracy_avg', accuracy_avg)\n",
    "summary_avg_loss = tf.summary.scalar('loss_avg', loss_avg)\n",
    "\n",
    "# Group summaries.\n",
    "summaries_training = tf.summary.merge([summary_trian_loss, summary_train_acc])\n",
    "summaries_evaluation = tf.summary.merge([summary_avg_accuracy, summary_avg_loss])\n",
    "\n",
    "# Generate a variable to contain a counter for the global training step.\n",
    "# Note that it is useful if you save/restore your network.\n",
    "global_step = tf.Variable(1, name='global_step', trainable=False)\n",
    "\n",
    "# Create optimization op.\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(config['learning_rate'])\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "#Create session object\n",
    "sess = tf.Session()\n",
    "# Add the ops to initialize variables.\n",
    "init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "# Actually intialize the variables\n",
    "sess.run(init_op)\n",
    "\n",
    "# Register summary ops.\n",
    "train_summary_dir = os.path.join(config['model_dir'], \"summary\", \"train\")\n",
    "train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "# Create a saver for writing training checkpoints.\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "# Define counters in order to accumulate measurements.\n",
    "counter_correct_predictions_training = 0.0\n",
    "counter_loss_training = 0.0\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100] [Training] Accuracy: 0.045, Loss: 3.088\n",
      "[200] [Training] Accuracy: 0.052, Loss: 3.015\n",
      "[300] [Training] Accuracy: 0.049, Loss: 3.005\n",
      "[400] [Training] Accuracy: 0.057, Loss: 2.999\n",
      "[500] [Training] Accuracy: 0.061, Loss: 2.994\n",
      "[600] [Training] Accuracy: 0.056, Loss: 2.993\n",
      "[700] [Training] Accuracy: 0.059, Loss: 2.993\n",
      "[800] [Training] Accuracy: 0.062, Loss: 2.991\n",
      "[900] [Training] Accuracy: 0.060, Loss: 2.990\n",
      "Model saved in file: /media/eaksan/Warehouse-SSD/Workspace/uie_ss17/assignment3/public/runs/1494252247/model-1000\n",
      "[1000] [Training] Accuracy: 0.069, Loss: 2.984\n",
      "[1100] [Training] Accuracy: 0.064, Loss: 2.979\n",
      "[1200] [Training] Accuracy: 0.067, Loss: 2.978\n",
      "[1300] [Training] Accuracy: 0.074, Loss: 2.969\n",
      "[1400] [Training] Accuracy: 0.072, Loss: 2.974\n",
      "[1500] [Training] Accuracy: 0.081, Loss: 2.960\n",
      "[1600] [Training] Accuracy: 0.077, Loss: 2.966\n",
      "[1700] [Training] Accuracy: 0.077, Loss: 2.950\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "try:\n",
    "    while not coord.should_stop():\n",
    "        step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "        if (step%config['checkpoint_every_step']) == 0:\n",
    "            ckpt_save_path = saver.save(sess, os.path.join(config['model_dir'], 'model'), global_step)\n",
    "            print(\"Model saved in file: %s\" % ckpt_save_path)\n",
    "\n",
    "        # This dictionary maps the batch data (as a numpy array) to the\n",
    "        # placeholder variables in the graph.\n",
    "        feed_dict = {}\n",
    "\n",
    "        # Run the optimizer to update weights.\n",
    "        # Note that \"train_op\" is responsible from updating network weights.\n",
    "        # Only the operations that are fed are evaluated.\n",
    "        # Run the optimizer to update weights.\n",
    "        train_summary, correct_predictions_training, loss_training, _ = sess.run([summaries_training, num_correct_predictions, loss, train_op], feed_dict=feed_dict)\n",
    "        # Update counters.\n",
    "        counter_correct_predictions_training += correct_predictions_training\n",
    "        counter_loss_training += loss_training\n",
    "        # Write summary data.\n",
    "        train_summary_writer.add_summary(train_summary, step)\n",
    "        \n",
    "        # Occasionally print status messages.\n",
    "        if (step%config['print_every_step']) == 0:\n",
    "            # Calculate average training accuracy.\n",
    "            accuracy_avg_value_training = counter_correct_predictions_training/(config['print_every_step']*config['batch_size'])\n",
    "            loss_avg_value_training = counter_loss_training/(config['print_every_step'])\n",
    "            # [Iteration]\n",
    "            print(\"[%d] [Training] Accuracy: %.3f, Loss: %.3f\" % (step, accuracy_avg_value_training, loss_avg_value_training))\n",
    "            counter_correct_predictions_training = 0.0\n",
    "            counter_loss_training = 0.0\n",
    "            # Report\n",
    "            # Note that accuracy_avg and loss_avg placeholders are defined\n",
    "            # just to feed average results to summaries.\n",
    "            summary_report = sess.run(summaries_evaluation, feed_dict={accuracy_avg:accuracy_avg_value_training, loss_avg:loss_avg_value_training})\n",
    "            train_summary_writer.add_summary(summary_report, step)\n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Done training for %d epochs, %d steps.' % (config['num_epochs'], step))\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()\n",
    "\n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)\n",
    "\n",
    "ckpt_save_path = saver.save(sess, os.path.join(config['model_dir'], 'model'), global_step)\n",
    "print(\"Model saved in file: %s\" % ckpt_save_path)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Samples: 16\n",
      "Sequence lengths: [62 88 62 63 82 98 73 63 57 71 76 59 73 63 61 52]\n",
      "Sequence labels: [12 11 17 16 18 13  4 13 19  8 14  2 18 14  1 10]\n",
      "Sequence type: <class 'numpy.ndarray'>\n",
      "Sequence shape: (16, 98, 1024)\n",
      "(flattened) Image shape: (1024,)\n",
      "Image shape: (32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9Xd90kI9SoICtQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95f4690cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "A simple sanity check for input pipeline:\n",
    "'''\n",
    "def test_input_loader():\n",
    "    config = {}\n",
    "    config['img_height'] = 80\n",
    "    config['img_width'] = 80\n",
    "    config['img_num_channels'] = 3\n",
    "    config['num_epochs'] = 10\n",
    "    config['batch_size'] = 16\n",
    "    # Capacity of the queue which contains the samples read by data readers.\n",
    "    # Make sure that it has enough capacity.\n",
    "    config['ip_queue_capacity'] = config['batch_size']*10  \n",
    "    config['ip_num_read_threads'] = 6\n",
    "    # Directory of the data.\n",
    "    config['data_dir'] = \"/home/eaksan/uie_data/train/\"\n",
    "    # File naming\n",
    "    config['file_format'] = \"dataTrain_%d.tfrecords\"\n",
    "    # File IDs to be used for training.\n",
    "    config['file_ids'] = list(range(1,10))\n",
    "    \n",
    "    # Create a list of TFRecord input files.\n",
    "    filenames = [os.path.join(config['data_dir'], config['file_format'] % i) for i in config['file_ids']]\n",
    "\n",
    "    # Create data loading operators. This will be represented as a node in the computational graph.\n",
    "    batch_samples_op, batch_labels_op, batch_seq_len_op = input_pipeline(filenames, config)\n",
    "    # TODO: batch_samples_op, batch_labels_op and batch_seq_len_op are like input placeholders. You can directly \n",
    "    # feed them to your model.\n",
    "\n",
    "    # Create tensorflow session and initialize the variables (if any).\n",
    "    sess = tf.Session()\n",
    "    init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    # Create threads to prefetch the data.\n",
    "    # https://www.tensorflow.org/programmers_guide/reading_data#creating_threads_to_prefetch_using_queuerunner_objects\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    batch_samples, batch_labels, batch_seq_len = sess.run([batch_samples_op, batch_labels_op, batch_seq_len_op])\n",
    "    \n",
    "    # Print \n",
    "    print(\"# Samples: \" + str(len(batch_samples)))\n",
    "    print(\"Sequence lengths: \" + str(batch_seq_len))\n",
    "    print(\"Sequence labels: \" + str(batch_labels))\n",
    "    \n",
    "    # Note that the second dimension will give maximum-length in the batch, i.e., the padded sequence length.\n",
    "    print(\"Sequence type: \" + str(type(batch_samples)))\n",
    "    print(\"Sequence shape: \" + str(batch_samples.shape))\n",
    "\n",
    "    # Fetch first clips 11th frame.\n",
    "    img = batch_samples[0][10]\n",
    "    print(\"(flattened) Image shape: \" + str(img.shape))\n",
    "    img = np.reshape(img, (32,32))\n",
    "    print(\"Image shape: \" + str(img.shape))\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img) # Note that image may look wierd because it is normalized.\n",
    "    \n",
    "test_input_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

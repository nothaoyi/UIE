{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "An example of TFRecord data loading, preprocessing and RNN training.\n",
    "    - Creates threads to read TFRecord files from disk, decode and preprocess.\n",
    "    - Crops and resizes the RGB frames, i.e., images, (32x32) and flatten: 1024 dimensional representation vector.\n",
    "    - Builds recurrent 2-layer LSTM model\n",
    "    - Trains the model on flattened image vectors.\n",
    "\n",
    "You can use 2D CNN for representation learning on images or 3D volumetric CNN on multiple frames. You should find out how to stack CNN and RNN networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /media/eaksan/Warehouse-SSD/Workspace/uie_ss17/3_project/public/runs/1495814183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "# Get from dataset.\n",
    "config['num_test_samples'] = 2174\n",
    "config['num_validation_samples'] = 1765\n",
    "config['num_training_samples'] = 5722\n",
    "\n",
    "config['batch_size'] = 32\n",
    "config['learning_rate'] = 1e-2\n",
    "config['learning_rate_type'] = 'exponential' # Learning rate is annealed exponentally.\n",
    "\n",
    "config['num_steps_per_epoch'] = int(config['num_training_samples']/config['batch_size'])\n",
    "\n",
    "config['num_epochs'] = 100\n",
    "config['evaluate_every_step'] = config['num_steps_per_epoch']*3\n",
    "config['checkpoint_every_step'] = config['num_steps_per_epoch']*10\n",
    "config['num_validation_steps'] = int(config['num_validation_samples']/config['batch_size'])\n",
    "config['print_every_step'] = config['num_steps_per_epoch']\n",
    "config['log_dir'] = './runs/'\n",
    "\n",
    "config['img_height'] = 80\n",
    "config['img_width'] = 80\n",
    "config['img_num_channels'] = 3\n",
    "config['skeleton_size'] = 180\n",
    "\n",
    "config['num_rnn_hidden_units'] = 512\n",
    "config['num_rnn_layers'] = 2\n",
    "config['num_class_labels'] = 20\n",
    "config['initializer'] = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "config['ip_queue_capacity'] = config['batch_size']*50\n",
    "config['ip_num_read_threads'] = 6\n",
    "\n",
    "config['train_data_dir'] = \"/home/eaksan/uie_data/train/\"\n",
    "config['train_file_format'] = \"dataTrain_%d.tfrecords\"\n",
    "config['train_file_ids'] = list(range(1,41))\n",
    "config['valid_data_dir'] = \"/home/eaksan/uie_data/validation/\"\n",
    "config['valid_file_format'] = \"dataValidation_%d.tfrecords\"\n",
    "config['valid_file_ids'] = list(range(1,16))\n",
    "\n",
    "\n",
    "# Create a unique output directory for this experiment.\n",
    "timestamp = str(int(time.time()))\n",
    "config['model_dir'] = os.path.abspath(os.path.join(config['log_dir'], timestamp))\n",
    "print(\"Writing to {}\\n\".format(config['model_dir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_op(image_op, config):\n",
    "    \"\"\"\n",
    "    Creates preprocessing operations that are going to be applied on a single frame.\n",
    "    \n",
    "    TODO: Customize for your needs.\n",
    "    You can do any preprocessing (masking, normalization/scaling of inputs, augmentation, etc.) by using tensorflow operations.\n",
    "    Built-in image operations: https://www.tensorflow.org/api_docs/python/tf/image \n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"preprocessing\"):\n",
    "        # Reshape serialized image.\n",
    "        image_op = tf.reshape(image_op, (config['img_height'], \n",
    "                                         config['img_width'], \n",
    "                                         config['img_num_channels'])\n",
    "                          )\n",
    "        # Convert from RGB to grayscale.\n",
    "        image_op = tf.image.rgb_to_grayscale(image_op)\n",
    "        \n",
    "        # Integer to float.\n",
    "        image_op = tf.to_float(image_op)\n",
    "        # Crop\n",
    "        image_op = tf.image.resize_image_with_crop_or_pad(image_op, 60, 60)\n",
    "        \n",
    "        # Resize operation requires 4D tensors (i.e., batch of images).\n",
    "        # Reshape the image so that it looks like a batch of one sample: [1,60,60,1]\n",
    "        image_op = tf.expand_dims(image_op, 0)\n",
    "        # Resize\n",
    "        image_op = tf.image.resize_bilinear(image_op, np.asarray([32,32]))\n",
    "        # Reshape the image: [32,32,1]\n",
    "        image_op = tf.squeeze(image_op, 0)\n",
    "        \n",
    "        # Normalize (zero-mean unit-variance) the image locally, i.e., by using statistics of the \n",
    "        # image not the whole data or sequence. \n",
    "        image_op = tf.image.per_image_standardization(image_op)\n",
    "        \n",
    "        # Flatten image\n",
    "        image_op = tf.reshape(image_op, [-1])\n",
    "    \n",
    "        return image_op\n",
    "\n",
    "def read_and_decode_sequence(filename_queue, config):\n",
    "    # Create a TFRecordReader.\n",
    "    readerOptions = tf.python_io.TFRecordOptions(compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    reader = tf.TFRecordReader(options=readerOptions)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    \n",
    "    # Read one sequence sample.\n",
    "    # The training and validation files contains the following fields:\n",
    "    # - label: label of the sequence which take values between 1 and 20.\n",
    "    # - length: length of the sequence, i.e., number of frames.\n",
    "    # - depth: sequence of depth images. [length x height x width x numChannels]\n",
    "    # - rgb: sequence of rgb images. [length x height x width x numChannels]\n",
    "    # - segmentation: sequence of segmentation maskes. [length x height x width x numChannels]\n",
    "    # - skeleton: sequence of flattened skeleton joint positions. [length x numJoints]\n",
    "    #\n",
    "    # The test files doesn't contain \"label\" field.\n",
    "    # [height, width, numChannels] = [80, 80, 3]\n",
    "    with tf.name_scope(\"TFRecordDecoding\"):\n",
    "        context_encoded, sequence_encoded = tf.parse_single_sequence_example(\n",
    "                serialized_example,\n",
    "                # \"label\" and \"lenght\" are encoded as context features. \n",
    "                context_features={\n",
    "                    \"label\": tf.FixedLenFeature([], dtype=tf.int64),\n",
    "                    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "                },\n",
    "                # \"depth\", \"rgb\", \"segmentation\", \"skeleton\" are encoded as sequence features.\n",
    "                sequence_features={\n",
    "                    \"depth\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"rgb\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"segmentation\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"skeleton\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                })\n",
    "\n",
    "        \n",
    "        # Fetch required data fields. \n",
    "        # TODO: Customize for your design. Assume that only the RGB images are used for now.\n",
    "        # Decode the serialized RGB images.    \n",
    "        seq_rgb = tf.decode_raw(sequence_encoded['rgb'], tf.uint8)\n",
    "        seq_label = context_encoded['label']\n",
    "        # Tensorflow requires the labels start from 0. Before you create submission csv, \n",
    "        # increment the predictions by 1.\n",
    "        seq_label = seq_label - 1\n",
    "        seq_len = tf.to_int32(context_encoded['length'])\n",
    "        # Output dimnesionality: [seq_len, height, width, numChannels]\n",
    "        # tf.map_fn applies the preprocessing function on every image in the sequence, i.e., frame.\n",
    "        seq_rgb = tf.map_fn(lambda x: preprocessing_op(x, config),\n",
    "                                elems=seq_rgb,\n",
    "                                dtype=tf.float32,\n",
    "                                back_prop=False)\n",
    "        \"\"\"\n",
    "        # Use skeleton only.\n",
    "        seq_skeleton = tf.decode_raw(sequence_encoded['skeleton'], tf.float32)\n",
    "        # Normalize skeleton so that every pose is a unit length vector.\n",
    "        seq_skeleton = tf.nn.l2_normalize(seq_skeleton, dim=1)\n",
    "        seq_skeleton.set_shape([None, config['skeleton_size']])\n",
    "        \n",
    "        seq_len = tf.to_int32(context_encoded['length'])\n",
    "        seq_label = context_encoded['label']\n",
    "        # Tensorflow requires the labels start from 0. Before you create submission csv, \n",
    "        # increment the predictions by 1.\n",
    "        seq_label = seq_label - 1\n",
    "        \"\"\"\n",
    "        \n",
    "        return [seq_rgb, seq_label, seq_len]\n",
    "    \n",
    "\n",
    "def input_pipeline(filenames, config, name='input_pipeline', shuffle=True):\n",
    "    with tf.name_scope(name):\n",
    "        # Create a queue of TFRecord input files.\n",
    "        filename_queue = tf.train.string_input_producer(filenames, num_epochs=config['num_epochs'], shuffle=shuffle)\n",
    "        # Read the data from TFRecord files, decode and create a list of data samples by using threads.\n",
    "        sample_list = [read_and_decode_sequence(filename_queue, config) for _ in range(config['ip_num_read_threads'])]\n",
    "        # Create batches.\n",
    "        # Since the data consists of variable-length sequences, allow padding by setting dynamic_pad parameter.\n",
    "        # \"batch_join\" creates batches of samples and pads the sequences w.r.t the max-length sequence in the batch.\n",
    "        # Hence, the padded sequence length can be different for different batches.\n",
    "        batch_rgb, batch_labels, batch_lens = tf.train.batch_join(sample_list,\n",
    "                                                    batch_size=config['batch_size'],\n",
    "                                                    capacity=config['ip_queue_capacity'],\n",
    "                                                    enqueue_many=False,\n",
    "                                                    dynamic_pad=True,\n",
    "                                                    allow_smaller_final_batch = False,\n",
    "                                                    name=\"batch_join_and_pad\")\n",
    "\n",
    "        return batch_rgb, batch_labels, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SequenceClassifier():\n",
    "    \"\"\"\n",
    "    Creates training and validation computational graphs.\n",
    "    Note that tf.variable_scope enables sharing the parameters so that both graphs share the parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, input_op, target_op, seq_len_op, mode):\n",
    "        \"\"\"\n",
    "        Basic setup.\n",
    "        Args:\n",
    "          config: Object containing configuration parameters.\n",
    "        \"\"\"\n",
    "        assert mode in [\"training\", \"validation\"]\n",
    "        self.config = config\n",
    "        self.inputs = input_op\n",
    "        self.targets = target_op\n",
    "        self.seq_lengths = seq_len_op\n",
    "        self.mode = mode\n",
    "        self.reuse = self.mode == \"validation\"\n",
    "        \n",
    "    def build_rnn_model(self):\n",
    "        with tf.variable_scope('rnn_cell', reuse=self.reuse, initializer=config['initializer']):\n",
    "            rnn_cell = tf.contrib.rnn.BasicLSTMCell(num_units=self.config['num_rnn_hidden_units'])\n",
    "        with tf.variable_scope('rnn_stack', reuse=self.reuse, initializer=config['initializer']):\n",
    "            if self.config['num_rnn_layers'] > 1:\n",
    "                rnn_cell = tf.contrib.rnn.MultiRNNCell([rnn_cell for _ in range(self.config['num_rnn_layers'])])\n",
    "            self.model_rnn, self.rnn_state = tf.nn.dynamic_rnn(\n",
    "                                            cell=rnn_cell,\n",
    "                                            inputs=self.inputs,\n",
    "                                            dtype = tf.float32,\n",
    "                                            sequence_length=self.seq_lengths,\n",
    "                                            time_major=False)\n",
    "            # Fetch output of the last step.\n",
    "            self.rnn_last_step = tf.gather_nd(self.model_rnn, tf.stack([tf.range(self.config['batch_size']), self.seq_lengths-1], axis=1))\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.build_rnn_model()\n",
    "        # Calculate logits\n",
    "        with tf.variable_scope('logits', reuse=self.reuse, initializer=config['initializer']):\n",
    "            self.logits = tf.layers.dense(inputs=self.rnn_last_step, units=self.config['num_class_labels'],\n",
    "                                            kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                            bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "    def loss(self):\n",
    "        # Loss calculations: cross-entropy\n",
    "        with tf.name_scope(\"cross_entropy_loss\"):\n",
    "            self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.targets))\n",
    "            \n",
    "                # Accuracy calculations.\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # Return list of predictions (useful for making a submission)\n",
    "            self.predictions = tf.argmax(self.logits, 1, name=\"predictions\")\n",
    "            # Return a bool tensor with shape [batch_size] that is true for the\n",
    "            # correct predictions.\n",
    "            self.correct_predictions = tf.equal(tf.argmax(self.logits, 1), self.targets)\n",
    "            # Number of correct predictions in order to calculate average accuracy afterwards.\n",
    "            self.num_correct_predictions = tf.reduce_sum(tf.cast(self.correct_predictions, tf.int32))\n",
    "            # Calculate the accuracy per minibatch.\n",
    "            self.batch_accuracy = tf.reduce_mean(tf.cast(self.correct_predictions, tf.float32))\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self.build_model()\n",
    "        self.loss()\n",
    "        self.num_parameters()\n",
    "        \n",
    "        return self.logits, self.loss, self.batch_accuracy, self.predictions\n",
    "    \n",
    "    def num_parameters(self):\n",
    "        self.num_parameters = 0\n",
    "        #iterating over all variables\n",
    "        for variable in tf.trainable_variables():\n",
    "            local_parameters=1\n",
    "            shape = variable.get_shape()  #getting shape of a variable\n",
    "            for i in shape:\n",
    "                local_parameters*=i.value  #mutiplying dimension values\n",
    "            self.num_parameters+=local_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# of parameters: 5257236\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tfRecord input files.\n",
    "train_filenames = [os.path.join(config['train_data_dir'], config['train_file_format'] % i) for i in config['train_file_ids']]\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "train_batch_samples_op, train_batch_labels_op, train_batch_seq_len_op = input_pipeline(train_filenames, config, name='training_input_pipeline')\n",
    "\n",
    "# Create a list of tfRecord input files.\n",
    "valid_filenames = [os.path.join(config['valid_data_dir'], config['valid_file_format'] % i) for i in config['valid_file_ids']]\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "valid_batch_samples_op, valid_batch_labels_op, valid_batch_seq_len_op = input_pipeline(valid_filenames, config, name='validation_input_pipeline', shuffle=False)\n",
    "\n",
    "# Create placeholders for training and monitoring variables.\n",
    "loss_avg_op = tf.placeholder(tf.float32, name=\"loss_avg\")\n",
    "accuracy_avg_op = tf.placeholder(tf.float32, name=\"accuracy_avg\")\n",
    "\n",
    "# Generate a variable to contain a counter for the global training step.\n",
    "# Note that it is useful if you save/restore your network.\n",
    "global_step = tf.Variable(1, name='global_step', trainable=False)\n",
    "\n",
    "# Create seperate graphs for training and validation.\n",
    "# Training graph\n",
    "# Note that our model is optimized by using the training graph.\n",
    "with tf.name_scope(\"Training\"):\n",
    "    # Create model\n",
    "    trainModel = SequenceClassifier(config=config, \n",
    "                                            input_op=train_batch_samples_op, \n",
    "                                            target_op=train_batch_labels_op, \n",
    "                                            seq_len_op=train_batch_seq_len_op,\n",
    "                                            mode=\"training\")\n",
    "    trainModel.build_graph()\n",
    "    print(\"\\n# of parameters: %s\" % trainModel.num_parameters)\n",
    "    \n",
    "    # Optimization routine.\n",
    "    learning_rate = config['learning_rate']\n",
    "    # Learning rate is decayed in time. This enables our model using higher learning rates in the beginning.\n",
    "    # In time the learning rate is decayed so that gradients don't explode and training staurates.\n",
    "    # If you observe slow training, feel free to modify decay_steps and decay_rate arguments.\n",
    "    if config['learning_rate_type'] == 'exponential':\n",
    "        learning_rate = tf.train.exponential_decay(config['learning_rate'], \n",
    "                                                   global_step=global_step,\n",
    "                                                   decay_steps=1000, \n",
    "                                                   decay_rate=0.97,\n",
    "                                                   staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(trainModel.loss, global_step=global_step)\n",
    "\n",
    "# Validation graph.\n",
    "with tf.name_scope(\"Evaluation\"):\n",
    "    # Create model\n",
    "    validModel = SequenceClassifier(config=config, \n",
    "                                            input_op=valid_batch_samples_op, \n",
    "                                            target_op=valid_batch_labels_op, \n",
    "                                            seq_len_op=valid_batch_seq_len_op,\n",
    "                                            mode=\"validation\")\n",
    "    validModel.build_graph()\n",
    "\n",
    "    \n",
    "# Create summary ops for monitoring the training.\n",
    "# Each summary op annotates a node in the computational graph and collects\n",
    "# data data from it.\n",
    "summary_train_loss = tf.summary.scalar('loss', trainModel.loss)\n",
    "summary_train_acc = tf.summary.scalar('accuracy_training', trainModel.batch_accuracy)\n",
    "summary_avg_accuracy = tf.summary.scalar('accuracy_avg', accuracy_avg_op)\n",
    "summary_avg_loss = tf.summary.scalar('loss_avg', loss_avg_op)\n",
    "summary_learning_rate = tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "# Group summaries.\n",
    "# summaries_training is used during training and reported after every step.\n",
    "summaries_training = tf.summary.merge([summary_train_loss, summary_train_acc, summary_learning_rate])\n",
    "# summaries_evaluation is used by both trainig and validation in order to report the performance on the dataset.\n",
    "summaries_evaluation = tf.summary.merge([summary_avg_accuracy, summary_avg_loss])\n",
    "    \n",
    "#Create session object\n",
    "sess = tf.Session()\n",
    "# Add the ops to initialize variables.\n",
    "init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "# Actually intialize the variables\n",
    "sess.run(init_op)\n",
    "\n",
    "# Register summary ops.\n",
    "train_summary_dir = os.path.join(config['model_dir'], \"summary\", \"train\")\n",
    "train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "valid_summary_dir = os.path.join(config['model_dir'], \"summary\", \"validation\")\n",
    "valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "\n",
    "# Create a saver for writing training checkpoints.\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "# Define counters in order to accumulate measurements.\n",
    "counter_correct_predictions_training = 0.0\n",
    "counter_loss_training = 0.0\n",
    "counter_correct_predictions_validation = 0.0\n",
    "counter_loss_validation = 0.0\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "try:\n",
    "    while not coord.should_stop():\n",
    "        step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "        if (step%config['checkpoint_every_step']) == 0:\n",
    "            ckpt_save_path = saver.save(sess, os.path.join(config['model_dir'], 'model'), global_step)\n",
    "            print(\"Model saved in file: %s\" % ckpt_save_path)\n",
    "            \n",
    "        # Run the optimizer to update weights.\n",
    "        # Note that \"train_op\" is responsible from updating network weights.\n",
    "        # Only the operations that are fed are evaluated.\n",
    "        # Run the optimizer to update weights.\n",
    "        train_summary, num_correct_predictions, loss, _ = sess.run([summaries_training, \n",
    "                                                                      trainModel.num_correct_predictions, \n",
    "                                                                      trainModel.loss, \n",
    "                                                                      train_op], \n",
    "                                                                      feed_dict={})\n",
    "        # Update counters.\n",
    "        counter_correct_predictions_training += num_correct_predictions\n",
    "        counter_loss_training += loss\n",
    "        # Write summary data.\n",
    "        train_summary_writer.add_summary(train_summary, step)\n",
    "        \n",
    "        # Report training performance\n",
    "        if (step%config['print_every_step']) == 0:\n",
    "            accuracy_avg = counter_correct_predictions_training / (config['batch_size']*config['print_every_step'])\n",
    "            loss_avg = counter_loss_training / (config['print_every_step'])\n",
    "            summary_report = sess.run(summaries_evaluation, feed_dict={accuracy_avg_op:accuracy_avg, loss_avg_op:loss_avg})\n",
    "            train_summary_writer.add_summary(summary_report, step)\n",
    "            print(\"[%d/%d] [Training] Accuracy: %.3f, Loss: %.3f\" % (step/config['num_steps_per_epoch'], \n",
    "                                                                     step, \n",
    "                                                                     accuracy_avg, \n",
    "                                                                     loss_avg))\n",
    "            \n",
    "            counter_correct_predictions_training = 0.0\n",
    "            counter_loss_training= 0.0\n",
    "        \n",
    "        if (step%config['evaluate_every_step']) == 0:\n",
    "            # It is possible to create only one input pipelene queue. Hence, we create a validation queue \n",
    "            # in the begining for multiple epochs and control it via a foor loop.\n",
    "            # Note that we only approximate 1 validation epoch (validation doesn't have to be accurate.)\n",
    "            # In other words, number of unique validation samples may differ everytime.\n",
    "            for eval_step in range(config['num_validation_steps']):\n",
    "                # Calculate average validation accuracy.\n",
    "                num_correct_predictions, loss = sess.run([validModel.num_correct_predictions, \n",
    "                                                          validModel.loss],\n",
    "                                                         feed_dict={})\n",
    "                # Update counters.\n",
    "                counter_correct_predictions_validation += num_correct_predictions\n",
    "                counter_loss_validation += loss\n",
    "            \n",
    "            # Report validation performance\n",
    "            accuracy_avg = counter_correct_predictions_validation / (config['batch_size']*config['num_validation_steps'])\n",
    "            loss_avg = counter_loss_validation / (config['num_validation_steps'])\n",
    "            summary_report = sess.run(summaries_evaluation, feed_dict={accuracy_avg_op:accuracy_avg, loss_avg_op:loss_avg})\n",
    "            valid_summary_writer.add_summary(summary_report, step)\n",
    "            print(\"[%d/%d] [Validation] Accuracy: %.3f, Loss: %.3f\" % (step/config['num_steps_per_epoch'], \n",
    "                                                                       step, \n",
    "                                                                       accuracy_avg, \n",
    "                                                                       loss_avg))\n",
    "            \n",
    "            counter_correct_predictions_validation = 0.0\n",
    "            counter_loss_validation= 0.0\n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Model is trained for %d epochs, %d steps.' % (config['num_epochs'], step))\n",
    "    print('Done.')\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()\n",
    "\n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)\n",
    "\n",
    "ckpt_save_path = saver.save(sess, os.path.join(config['model_dir'], 'model'), global_step)\n",
    "print(\"Model saved in file: %s\" % ckpt_save_path)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A simple sanity check for input pipeline:\n",
    "'''\n",
    "def test_input_loader():\n",
    "    config = {}\n",
    "    config['img_height'] = 80\n",
    "    config['img_width'] = 80\n",
    "    config['img_num_channels'] = 3\n",
    "    config['num_epochs'] = 10\n",
    "    config['batch_size'] = 16\n",
    "    # Capacity of the queue which contains the samples read by data readers.\n",
    "    # Make sure that it has enough capacity.\n",
    "    config['ip_queue_capacity'] = config['batch_size']*10  \n",
    "    config['ip_num_read_threads'] = 6\n",
    "    # Directory of the data.\n",
    "    config['data_dir'] = \"/home/eaksan/uie_data/train/\"\n",
    "    # File naming\n",
    "    config['file_format'] = \"dataTrain_%d.tfrecords\"\n",
    "    # File IDs to be used for training.\n",
    "    config['file_ids'] = list(range(1,10))\n",
    "    \n",
    "    # Create a list of TFRecord input files.\n",
    "    filenames = [os.path.join(config['data_dir'], config['file_format'] % i) for i in config['file_ids']]\n",
    "\n",
    "    # Create data loading operators. This will be represented as a node in the computational graph.\n",
    "    batch_samples_op, batch_labels_op, batch_seq_len_op = input_pipeline(filenames, config)\n",
    "    # TODO: batch_samples_op, batch_labels_op and batch_seq_len_op are like input placeholders. You can directly \n",
    "    # feed them to your model.\n",
    "\n",
    "    # Create tensorflow session and initialize the variables (if any).\n",
    "    sess = tf.Session()\n",
    "    init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    # Create threads to prefetch the data.\n",
    "    # https://www.tensorflow.org/programmers_guide/reading_data#creating_threads_to_prefetch_using_queuerunner_objects\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    batch_samples, batch_labels, batch_seq_len = sess.run([batch_samples_op, batch_labels_op, batch_seq_len_op])\n",
    "    \n",
    "    # Print \n",
    "    print(\"# Samples: \" + str(len(batch_samples)))\n",
    "    print(\"Sequence lengths: \" + str(batch_seq_len))\n",
    "    print(\"Sequence labels: \" + str(batch_labels))\n",
    "    \n",
    "    # Note that the second dimension will give maximum-length in the batch, i.e., the padded sequence length.\n",
    "    print(\"Sequence type: \" + str(type(batch_samples)))\n",
    "    print(\"Sequence shape: \" + str(batch_samples.shape))\n",
    "\n",
    "    # Fetch first clips 11th frame.\n",
    "    img = batch_samples[0][10]\n",
    "    print(\"(flattened) Image shape: \" + str(img.shape))\n",
    "    img = np.reshape(img, (32,32))\n",
    "    print(\"Image shape: \" + str(img.shape))\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img) # Note that image may look wierd because it is normalized.\n",
    "    \n",
    "test_input_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Converts TFRecord data into numpy. Feel free to modify based on your needs.\n",
    "\n",
    "Data is saved into pickle files. Every file contains a list of samples. # of the samples in a file can be set via config['num_samples_in_numpy_list']. \n",
    "\n",
    "Loading:\n",
    "data = pickle.load(open(<i>path-to-pkl-file</i>, 'rb'))\n",
    "\n",
    "Each sample is a dictionary with the following fields:\n",
    "<ol>\n",
    "  <li>'label': label of the gesture. A unique ID in {0,1,..,19},</li>\n",
    "  <li>'length': length of the gesture sequence, i.e., # of frames,</li>\n",
    "  <li>'depth': tensor of depth images (length, height, width, 1),</li>\n",
    "  <li>'skeleton': tensor of skeleton joints (length, 180),</li>\n",
    "  <li>'rgb': tensor of rgb images (length, height, width, 3),</li>\n",
    "  <li>'segmentation': tensor of segmentation masks (length, height, width, 3).</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "Note that samples have different number of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_op(image_op, shape):\n",
    "    \"\"\"\n",
    "    Creates preprocessing operations that are going to be applied on a single frame.\n",
    "    \n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"preprocessing\"):\n",
    "        # Reshape serialized image.\n",
    "        return tf.reshape(image_op, shape)\n",
    "\n",
    "def read_and_decode_sequence(filename_queue, config):\n",
    "    # Create a TFRecordReader.\n",
    "    readerOptions = tf.python_io.TFRecordOptions(compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    reader = tf.TFRecordReader(options=readerOptions)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    \n",
    "    # Read one sequence sample.\n",
    "    # The training and validation files contains the following fields:\n",
    "    # - label: label of the sequence which take values between 1 and 20.\n",
    "    # - length: length of the sequence, i.e., number of frames.\n",
    "    # - depth: sequence of depth images. [length x height x width x numChannels]\n",
    "    # - rgb: sequence of rgb images. [length x height x width x numChannels]\n",
    "    # - segmentation: sequence of segmentation maskes. [length x height x width x numChannels]\n",
    "    # - skeleton: sequence of flattened skeleton joint positions. [length x numJoints]\n",
    "    #\n",
    "    # The test files doesn't contain \"label\" field.\n",
    "    with tf.name_scope(\"TFRecordDecoding\"):\n",
    "        context_encoded, sequence_encoded = tf.parse_single_sequence_example(\n",
    "                serialized_example,\n",
    "                # \"label\" and \"lenght\" are encoded as context features. \n",
    "                context_features={\n",
    "                    \"label\": tf.FixedLenFeature([], dtype=tf.int64),\n",
    "                    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "                },\n",
    "                # \"depth\", \"rgb\", \"segmentation\", \"skeleton\" are encoded as sequence features.\n",
    "                sequence_features={\n",
    "                    \"depth\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"rgb\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"segmentation\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"skeleton\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                })\n",
    "\n",
    "        # Fetch data fields.\n",
    "        seq_rgb = tf.decode_raw(sequence_encoded['rgb'], tf.uint8)\n",
    "        seq_depth = tf.decode_raw(sequence_encoded['depth'], tf.uint8)\n",
    "        seq_segmentation = tf.decode_raw(sequence_encoded['segmentation'], tf.uint8)\n",
    "        \n",
    "        # Output dimnesionality: [seq_len, height, width, numChannels]\n",
    "        # tf.map_fn applies the preprocessing function on every image in the sequence, i.e., frame.\n",
    "        seq_rgb = tf.map_fn(lambda x: preprocessing_op(x, (config['img_height'], config['img_width'], config['img_num_channels'])),\n",
    "                                elems=seq_rgb,\n",
    "                                dtype=tf.uint8,\n",
    "                                back_prop=False)\n",
    "        seq_depth = tf.map_fn(lambda x: preprocessing_op(x, (config['img_height'], config['img_width'], 1)),\n",
    "                                elems=seq_depth,\n",
    "                                dtype=tf.uint8,\n",
    "                                back_prop=False)\n",
    "        \n",
    "        seq_segmentation = tf.map_fn(lambda x: preprocessing_op(x, (config['img_height'], config['img_width'], config['img_num_channels'])),\n",
    "                                elems=seq_segmentation,\n",
    "                                dtype=tf.uint8,\n",
    "                                back_prop=False)\n",
    "        \n",
    "        seq_label = context_encoded['label']\n",
    "        seq_len = tf.to_int32(context_encoded['length'])\n",
    "        \n",
    "        #[batch_size, seq_len, num_skeleton_joints]\n",
    "        seq_skeleton = tf.decode_raw(sequence_encoded['skeleton'], tf.float32)\n",
    "    \n",
    "        return [seq_rgb, seq_depth, seq_segmentation, seq_skeleton, seq_label, seq_len]\n",
    "    \n",
    "def input_pipeline(filenames, config):\n",
    "    with tf.name_scope(\"input_pipeline\"):\n",
    "        # Create a queue of TFRecord input files.\n",
    "        filename_queue = tf.train.string_input_producer(filenames, num_epochs=config['num_epochs'], shuffle=False)\n",
    "        # Read the data from TFRecord files, decode and create a list of data samples by using threads.\n",
    "        sample_list = [read_and_decode_sequence(filename_queue, config) for _ in range(config['ip_num_read_threads'])]\n",
    "        # Create batches.\n",
    "        batch_rgb, batch_depth, batch_segmentation,batch_skeleton, \\\n",
    "                    batch_labels, batch_lens = tf.train.batch_join(sample_list,\n",
    "                                                                    batch_size=config['batch_size'],\n",
    "                                                                    capacity=config['ip_queue_capacity'],\n",
    "                                                                    enqueue_many=False,\n",
    "                                                                    dynamic_pad=True,\n",
    "                                                                    allow_smaller_final_batch=True,\n",
    "                                                                    name=\"batch_join_and_pad\")\n",
    "\n",
    "        return batch_rgb, batch_depth, batch_segmentation, batch_skeleton, batch_labels, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = {}\n",
    "# TODO: You can change these fields.\n",
    "config['input_dir'] = \"/home/eaksan/uie_data/train/\" # Directory of the tfrecords.\n",
    "config['input_file_format'] = \"dataTrain_%d.tfrecords\" # File naming\n",
    "config['input_file_ids'] = list(range(1,5)) # File IDs to be used for training.\n",
    "\n",
    "config['num_samples_in_numpy_list'] = 100 # Put 100 samples in a pickle data file. You can put everything in a single file as well.\n",
    "config['output_dir'] = config['input_dir']\n",
    "config['output_file_format'] = config['input_file_format'].split(\".\")[0]+\".pkl\"\n",
    "config['output_file_start_id'] = 1\n",
    "\n",
    "# Keep these fields fixed.\n",
    "config['img_height'] = 80\n",
    "config['img_width'] = 80\n",
    "config['img_num_channels'] = 3\n",
    "config['num_epochs'] = 1\n",
    "config['batch_size'] = 1\n",
    "# Capacity of the queue which contains the samples read by data readers.\n",
    "# Make sure that it has enough capacity.\n",
    "config['ip_queue_capacity'] = config['batch_size']*10  \n",
    "config['ip_num_read_threads'] = 1\n",
    "# Create a list of TFRecord input files.\n",
    "filenames = [os.path.join(config['input_dir'], config['input_file_format'] % i) for i in config['input_file_ids']]\n",
    "\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "rgb_op, depth_op, segmentation_op, skeleton_op, label_op, seq_len_op = input_pipeline(filenames, config)\n",
    "\n",
    "# Create tensorflow session and initialize the variables (if any).\n",
    "sess = tf.Session()\n",
    "init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "# Create threads to prefetch the data.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "np_file_id = config['output_file_start_id']\n",
    "output_list = []\n",
    "num_samples_read = 0\n",
    "try:\n",
    "    while not coord.should_stop():\n",
    "        rgb, depth, segmentation, skeleton, label, seq_len = sess.run([rgb_op, depth_op, segmentation_op, skeleton_op, label_op, seq_len_op])\n",
    "        num_samples_read += 1\n",
    "        data_sample = {}\n",
    "        data_sample['rgb'] = rgb[0] # Data is in batch format. Get rid of the first dimension.\n",
    "        data_sample['depth'] = depth[0]\n",
    "        data_sample['segmentation'] = segmentation[0]\n",
    "        data_sample['skeleton'] = skeleton[0]\n",
    "        data_sample['label'] = label[0]\n",
    "        data_sample['length'] = seq_len[0]\n",
    "        output_list.append(data_sample)\n",
    "        \n",
    "        if num_samples_read%config['num_samples_in_numpy_list'] == 0:\n",
    "            pickle.dump(output_list, open(os.path.join(config['output_dir'], config['output_file_format'] % np_file_id), 'wb'))\n",
    "            np_file_id += 1\n",
    "            output_list = []\n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    # Save last run.\n",
    "    if len(output_list) > 0:\n",
    "        pickle.dump(output_list, open(os.path.join(config['output_dir'], config['output_file_format'] % np_file_id), 'wb'))\n",
    "        output_list = []\n",
    "    print('Done.')\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()\n",
    "\n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

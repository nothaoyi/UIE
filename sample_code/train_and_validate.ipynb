{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "An example of TFRecord data loading, preprocessing and RNN training.\n",
    "    - Creates threads to read TFRecord files from disk, decode and preprocess.\n",
    "    - Crops and resizes the RGB frames, i.e., images, (32x32) and flatten: 1024 dimensional representation vector.\n",
    "    - Builds recurrent 2-layer LSTM model\n",
    "    - Trains the model on flattened image vectors.\n",
    "\n",
    "You can use 2D CNN for representation learning on images or 3D volumetric CNN on multiple frames. You should find out how to stack CNN and RNN networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from model_input import  input_pipeline\n",
    "from model import CNNModel, RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /media/eaksan/Warehouse-SSD/Workspace/uie_ss17/3_project/public/runs/1497039491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note that the evaluation code should have the same configuration.\n",
    "config = {}\n",
    "# Get from dataset.\n",
    "config['num_test_samples'] = 2174\n",
    "config['num_validation_samples'] = 1765\n",
    "config['num_training_samples'] = 5722\n",
    "\n",
    "config['batch_size'] = 16\n",
    "config['learning_rate'] = 1e-3\n",
    "# Learning rate is annealed exponentally in 'exponential' case. Don't forget to change annealing configuration in the code.\n",
    "config['learning_rate_type'] = 'fixed' #'exponential'\n",
    "\n",
    "config['num_steps_per_epoch'] = int(config['num_training_samples']/config['batch_size'])\n",
    "\n",
    "config['num_epochs'] = 1000\n",
    "config['evaluate_every_step'] = config['num_steps_per_epoch']*3\n",
    "config['checkpoint_every_step'] = config['num_steps_per_epoch']*10\n",
    "config['num_validation_steps'] = int(config['num_validation_samples']/config['batch_size'])\n",
    "config['print_every_step'] = config['num_steps_per_epoch']\n",
    "config['log_dir'] = './runs/'\n",
    "\n",
    "config['img_height'] = 80\n",
    "config['img_width'] = 80\n",
    "config['img_num_channels'] = 3\n",
    "config['skeleton_size'] = 180\n",
    "\n",
    "# CNN model parameters\n",
    "config['cnn'] = {}\n",
    "config['cnn']['cnn_filters'] = [16,32,64,128] # Number of filters for every convolutional layer.\n",
    "config['cnn']['num_hidden_units'] = 512 # Number of output units, i.e. representation size.\n",
    "config['cnn']['dropout_rate'] = 0.5\n",
    "config['cnn']['initializer'] = tf.contrib.layers.xavier_initializer()\n",
    "# RNN model parameters\n",
    "config['rnn'] = {}\n",
    "config['rnn']['num_hidden_units'] = 512 # Number of units in an LSTM cell.\n",
    "config['rnn']['num_layers'] = 1 # Number of LSTM stack.\n",
    "config['rnn']['num_class_labels'] = 20\n",
    "config['rnn']['initializer'] = tf.contrib.layers.xavier_initializer()\n",
    "config['rnn']['batch_size'] = config['batch_size']\n",
    "config['rnn']['loss_type'] = 'average' # or 'last_step' # In the case of 'average', average of all time-steps is used instead of the last time-step.\n",
    "\n",
    "config['ip_queue_capacity'] = config['batch_size']*50\n",
    "config['ip_num_read_threads'] = 6\n",
    "\n",
    "config['train_data_dir'] = \"/home/eaksan/uie_data/train/\"\n",
    "config['train_file_format'] = \"dataTrain_%d.tfrecords\"\n",
    "config['train_file_ids'] = list(range(1,41))\n",
    "config['valid_data_dir'] = \"/home/eaksan/uie_data/validation/\"\n",
    "config['valid_file_format'] = \"dataValidation_%d.tfrecords\"\n",
    "config['valid_file_ids'] = list(range(1,16))\n",
    "\n",
    "\n",
    "# Create a unique output directory for this experiment.\n",
    "timestamp = str(int(time.time()))\n",
    "config['model_dir'] = os.path.abspath(os.path.join(config['log_dir'], timestamp))\n",
    "print(\"Writing to {}\\n\".format(config['model_dir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# of parameters: 3853716\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1024,2048]\n\t [[Node: rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform = Add[T=DT_FLOAT, _class=[\"loc:@rnn_stack/rnn/basic_lstm_cell/weights\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform/mul, rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform/min)]]\n\nCaused by op 'rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-09e4d621f735>\", line 34, in <module>\n    trainModel.build_graph()\n  File \"/media/eaksan/Warehouse-SSD/Workspace/uie_ss17/3_project/public/model.py\", line 193, in build_graph\n    self.build_model()\n  File \"/media/eaksan/Warehouse-SSD/Workspace/uie_ss17/3_project/public/model.py\", line 160, in build_model\n    self.build_rnn_model()\n  File \"/media/eaksan/Warehouse-SSD/Workspace/uie_ss17/3_project/public/model.py\", line 148, in build_rnn_model\n    swap_memory=True)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\", line 545, in dynamic_rnn\n    dtype=dtype)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\", line 712, in _dynamic_rnn_loop\n    swap_memory=swap_memory)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2626, in while_loop\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2459, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2409, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\", line 695, in _time_step\n    skip_conditionals=True)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\", line 177, in _rnn_step\n    new_output, new_state = call_cell()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\", line 683, in <lambda>\n    call_cell = lambda: cell(input_t, state)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 179, in __call__\n    concat = _linear([inputs, h], 4 * self._num_units, True, scope=scope)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 747, in _linear\n    \"weights\", [total_arg_size, output_size], dtype=dtype)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 988, in get_variable\n    custom_getter=custom_getter)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 890, in get_variable\n    custom_getter=custom_getter)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 348, in get_variable\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 333, in _true_getter\n    caching_device=caching_device, validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 684, in _get_single_variable\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 226, in __init__\n    expected_shape=expected_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 303, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 673, in <lambda>\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/initializers.py\", line 145, in _initializer\n    dtype, seed=seed)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/random_ops.py\", line 246, in random_uniform\n    return math_ops.add(rnd * (maxval - minval), minval, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 73, in add\n    result = _op_def_lib.apply_op(\"Add\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1024,2048]\n\t [[Node: rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform = Add[T=DT_FLOAT, _class=[\"loc:@rnn_stack/rnn/basic_lstm_cell/weights\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform/mul, rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform/min)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1024,2048]\n\t [[Node: rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform = Add[T=DT_FLOAT, _class=[\"loc:@rnn_stack/rnn/basic_lstm_cell/weights\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform/mul, rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform/min)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-09e4d621f735>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0minit_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Actually intialize the variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Register summary ops.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1024,2048]\n\t [[Node: rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform = Add[T=DT_FLOAT, _class=[\"loc:@rnn_stack/rnn/basic_lstm_cell/weights\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform/mul, rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform/min)]]\n\nCaused by op 'rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-09e4d621f735>\", line 34, in <module>\n    trainModel.build_graph()\n  File \"/media/eaksan/Warehouse-SSD/Workspace/uie_ss17/3_project/public/model.py\", line 193, in build_graph\n    self.build_model()\n  File \"/media/eaksan/Warehouse-SSD/Workspace/uie_ss17/3_project/public/model.py\", line 160, in build_model\n    self.build_rnn_model()\n  File \"/media/eaksan/Warehouse-SSD/Workspace/uie_ss17/3_project/public/model.py\", line 148, in build_rnn_model\n    swap_memory=True)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\", line 545, in dynamic_rnn\n    dtype=dtype)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\", line 712, in _dynamic_rnn_loop\n    swap_memory=swap_memory)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2626, in while_loop\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2459, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2409, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\", line 695, in _time_step\n    skip_conditionals=True)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\", line 177, in _rnn_step\n    new_output, new_state = call_cell()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\", line 683, in <lambda>\n    call_cell = lambda: cell(input_t, state)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 179, in __call__\n    concat = _linear([inputs, h], 4 * self._num_units, True, scope=scope)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 747, in _linear\n    \"weights\", [total_arg_size, output_size], dtype=dtype)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 988, in get_variable\n    custom_getter=custom_getter)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 890, in get_variable\n    custom_getter=custom_getter)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 348, in get_variable\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 333, in _true_getter\n    caching_device=caching_device, validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 684, in _get_single_variable\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 226, in __init__\n    expected_shape=expected_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 303, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 673, in <lambda>\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/initializers.py\", line 145, in _initializer\n    dtype, seed=seed)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/random_ops.py\", line 246, in random_uniform\n    return math_ops.add(rnd * (maxval - minval), minval, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 73, in add\n    result = _op_def_lib.apply_op(\"Add\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1024,2048]\n\t [[Node: rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform = Add[T=DT_FLOAT, _class=[\"loc:@rnn_stack/rnn/basic_lstm_cell/weights\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform/mul, rnn_stack/rnn/basic_lstm_cell/weights/Initializer/random_uniform/min)]]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tfRecord input files.\n",
    "train_filenames = [os.path.join(config['train_data_dir'], config['train_file_format'] % i) for i in config['train_file_ids']]\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "train_batch_samples_op, train_batch_labels_op, train_batch_seq_len_op = input_pipeline(train_filenames, config, name='training_input_pipeline', mode=\"training\")\n",
    "\n",
    "# Create a list of tfRecord input files.\n",
    "valid_filenames = [os.path.join(config['valid_data_dir'], config['valid_file_format'] % i) for i in config['valid_file_ids']]\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "valid_batch_samples_op, valid_batch_labels_op, valid_batch_seq_len_op = input_pipeline(valid_filenames, config, name='validation_input_pipeline', shuffle=False, mode=\"training\")\n",
    "\n",
    "# Create placeholders for training and monitoring variables.\n",
    "loss_avg_op = tf.placeholder(tf.float32, name=\"loss_avg\")\n",
    "accuracy_avg_op = tf.placeholder(tf.float32, name=\"accuracy_avg\")\n",
    "\n",
    "# Generate a variable to contain a counter for the global training step.\n",
    "# Note that it is useful if you save/restore your network.\n",
    "global_step = tf.Variable(1, name='global_step', trainable=False)\n",
    "\n",
    "# Create seperate graphs for training and validation.\n",
    "# Training graph\n",
    "# Note that our model is optimized by using the training graph.\n",
    "with tf.name_scope(\"Training\"):\n",
    "    # Create model\n",
    "    cnnModel = CNNModel(config=config['cnn'],\n",
    "                        input_op=train_batch_samples_op, \n",
    "                        mode='training')\n",
    "    cnn_representations = cnnModel.build_graph()\n",
    "    \n",
    "    trainModel = RNNModel(config=config['rnn'], \n",
    "                            input_op=cnn_representations, \n",
    "                            target_op=train_batch_labels_op, \n",
    "                            seq_len_op=train_batch_seq_len_op,\n",
    "                            mode=\"training\")\n",
    "    trainModel.build_graph()\n",
    "    print(\"\\n# of parameters: %s\" % trainModel.num_parameters)\n",
    "    \n",
    "    # Optimization routine.\n",
    "    # Learning rate is decayed in time. This enables our model using higher learning rates in the beginning.\n",
    "    # In time the learning rate is decayed so that gradients don't explode and training staurates.\n",
    "    # If you observe slow training, feel free to modify decay_steps and decay_rate arguments.\n",
    "    if config['learning_rate_type'] == 'exponential':\n",
    "        learning_rate = tf.train.exponential_decay(config['learning_rate'], \n",
    "                                                   global_step=global_step,\n",
    "                                                   decay_steps=1000, \n",
    "                                                   decay_rate=0.97,\n",
    "                                                   staircase=False)\n",
    "    elif config['learning_rate_type'] == 'fixed':\n",
    "        learning_rate = config['learning_rate']\n",
    "    else:\n",
    "        print(\"Invalid learning rate type\")\n",
    "        raise\n",
    "        \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(trainModel.loss, global_step=global_step)\n",
    "\n",
    "# Validation graph.\n",
    "with tf.name_scope(\"Evaluation\"):\n",
    "    # Create model\n",
    "    validCnnModel = CNNModel(config=config['cnn'],\n",
    "                                input_op=valid_batch_samples_op, \n",
    "                                mode='validation')\n",
    "    valid_cnn_representations = validCnnModel.build_graph()\n",
    "    \n",
    "    validModel = RNNModel(config=config['rnn'], \n",
    "                            input_op=valid_cnn_representations, \n",
    "                            target_op=valid_batch_labels_op, \n",
    "                            seq_len_op=valid_batch_seq_len_op,\n",
    "                            mode=\"validation\")\n",
    "    validModel.build_graph()\n",
    "    \n",
    "# Create summary ops for monitoring the training.\n",
    "# Each summary op annotates a node in the computational graph and collects\n",
    "# data data from it.\n",
    "summary_train_loss = tf.summary.scalar('loss', trainModel.loss)\n",
    "summary_train_acc = tf.summary.scalar('accuracy_training', trainModel.batch_accuracy)\n",
    "summary_avg_accuracy = tf.summary.scalar('accuracy_avg', accuracy_avg_op)\n",
    "summary_avg_loss = tf.summary.scalar('loss_avg', loss_avg_op)\n",
    "summary_learning_rate = tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "# Group summaries.\n",
    "# summaries_training is used during training and reported after every step.\n",
    "summaries_training = tf.summary.merge([summary_train_loss, summary_train_acc, summary_learning_rate])\n",
    "# summaries_evaluation is used by both trainig and validation in order to report the performance on the dataset.\n",
    "summaries_evaluation = tf.summary.merge([summary_avg_accuracy, summary_avg_loss])\n",
    "    \n",
    "#Create session object\n",
    "sess = tf.Session()\n",
    "# Add the ops to initialize variables.\n",
    "init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "# Actually intialize the variables\n",
    "sess.run(init_op)\n",
    "\n",
    "# Register summary ops.\n",
    "train_summary_dir = os.path.join(config['model_dir'], \"summary\", \"train\")\n",
    "train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "valid_summary_dir = os.path.join(config['model_dir'], \"summary\", \"validation\")\n",
    "valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "\n",
    "# Create a saver for writing training checkpoints.\n",
    "saver = tf.train.Saver(var_list=tf.trainable_variables(), max_to_keep=3)\n",
    "\n",
    "# Define counters in order to accumulate measurements.\n",
    "counter_correct_predictions_training = 0.0\n",
    "counter_loss_training = 0.0\n",
    "counter_correct_predictions_validation = 0.0\n",
    "counter_loss_validation = 0.0\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "try:\n",
    "    while not coord.should_stop():\n",
    "        step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "        if (step%config['checkpoint_every_step']) == 0:\n",
    "            ckpt_save_path = saver.save(sess, os.path.join(config['model_dir'], 'model'), global_step)\n",
    "            print(\"Model saved in file: %s\" % ckpt_save_path)\n",
    "            \n",
    "        # Run the optimizer to update weights.\n",
    "        # Note that \"train_op\" is responsible from updating network weights.\n",
    "        # Only the operations that are fed are evaluated.\n",
    "        # Run the optimizer to update weights.\n",
    "        train_summary, num_correct_predictions, loss, _ = sess.run([summaries_training, \n",
    "                                                                      trainModel.num_correct_predictions, \n",
    "                                                                      trainModel.loss, \n",
    "                                                                      train_op], \n",
    "                                                                      feed_dict={})\n",
    "        # Update counters.\n",
    "        counter_correct_predictions_training += num_correct_predictions\n",
    "        counter_loss_training += loss\n",
    "        # Write summary data.\n",
    "        train_summary_writer.add_summary(train_summary, step)\n",
    "        \n",
    "        # Report training performance\n",
    "        if (step%config['print_every_step']) == 0:\n",
    "            accuracy_avg = counter_correct_predictions_training / (config['batch_size']*config['print_every_step'])\n",
    "            loss_avg = counter_loss_training / (config['print_every_step'])\n",
    "            summary_report = sess.run(summaries_evaluation, feed_dict={accuracy_avg_op:accuracy_avg, loss_avg_op:loss_avg})\n",
    "            train_summary_writer.add_summary(summary_report, step)\n",
    "            print(\"[%d/%d] [Training] Accuracy: %.3f, Loss: %.3f\" % (step/config['num_steps_per_epoch'], \n",
    "                                                                     step, \n",
    "                                                                     accuracy_avg, \n",
    "                                                                     loss_avg))\n",
    "            \n",
    "            counter_correct_predictions_training = 0.0\n",
    "            counter_loss_training= 0.0\n",
    "        \n",
    "        if (step%config['evaluate_every_step']) == 0:\n",
    "            # It is possible to create only one input pipelene queue. Hence, we create a validation queue \n",
    "            # in the begining for multiple epochs and control it via a foor loop.\n",
    "            # Note that we only approximate 1 validation epoch (validation doesn't have to be accurate.)\n",
    "            # In other words, number of unique validation samples may differ everytime.\n",
    "            for eval_step in range(config['num_validation_steps']):\n",
    "                # Calculate average validation accuracy.\n",
    "                num_correct_predictions, loss = sess.run([validModel.num_correct_predictions, \n",
    "                                                          validModel.loss],\n",
    "                                                         feed_dict={})\n",
    "                # Update counters.\n",
    "                counter_correct_predictions_validation += num_correct_predictions\n",
    "                counter_loss_validation += loss\n",
    "            \n",
    "            # Report validation performance\n",
    "            accuracy_avg = counter_correct_predictions_validation / (config['batch_size']*config['num_validation_steps'])\n",
    "            loss_avg = counter_loss_validation / (config['num_validation_steps'])\n",
    "            summary_report = sess.run(summaries_evaluation, feed_dict={accuracy_avg_op:accuracy_avg, loss_avg_op:loss_avg})\n",
    "            valid_summary_writer.add_summary(summary_report, step)\n",
    "            print(\"[%d/%d] [Validation] Accuracy: %.3f, Loss: %.3f\" % (step/config['num_steps_per_epoch'], \n",
    "                                                                       step, \n",
    "                                                                       accuracy_avg, \n",
    "                                                                       loss_avg))\n",
    "            \n",
    "            counter_correct_predictions_validation = 0.0\n",
    "            counter_loss_validation= 0.0\n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Model is trained for %d epochs, %d steps.' % (config['num_epochs'], step))\n",
    "    print('Done.')\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()\n",
    "\n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)\n",
    "\n",
    "ckpt_save_path = saver.save(sess, os.path.join(config['model_dir'], 'model'), global_step)\n",
    "print(\"Model saved in file: %s\" % ckpt_save_path)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

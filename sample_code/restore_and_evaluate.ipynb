{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "An example of TFRecord data loading, preprocessing and RNN training.\n",
    "    - Creates threads to read TFRecord files from disk, decode and preprocess.\n",
    "    - Crops and resizes the RGB frames, i.e., images, (32x32) and flatten: 1024 dimensional representation vector.\n",
    "    - Builds recurrent 2-layer LSTM model\n",
    "    - Trains the model on flattened image vectors.\n",
    "\n",
    "You can use 2D CNN for representation learning on images or 3D volumetric CNN on multiple frames. You should find out how to stack CNN and RNN networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from model_input import  input_pipeline\n",
    "from model import CNNModel, RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "# Get from dataset.\n",
    "config['num_test_samples'] = 2174\n",
    "config['batch_size'] = 16\n",
    "\n",
    "config['num_epochs'] = 1\n",
    "config['model_dir'] = '/home/nico/git/uieproject/sample_code/runs/1497291060/'\n",
    "config['checkpoint_id'] = None # If None, the last checkpoint will be used.\n",
    "\n",
    "config['img_height'] = 80\n",
    "config['img_width'] = 80\n",
    "config['img_num_channels'] = 3\n",
    "config['skeleton_size'] = 180\n",
    "\n",
    "# CNN model parameters\n",
    "config['cnn'] = {}\n",
    "config['cnn']['cnn_filters'] = [16,32,64,128] # Number of filters for every convolutional layer.\n",
    "config['cnn']['num_hidden_units'] = 512 # Number of output units, i.e. representation size.\n",
    "config['cnn']['dropout_rate'] = 0.5\n",
    "config['cnn']['initializer'] = tf.contrib.layers.xavier_initializer()\n",
    "# RNN model parameters\n",
    "config['rnn'] = {}\n",
    "config['rnn']['num_hidden_units'] = 512 # Number of units in an LSTM cell.\n",
    "config['rnn']['num_layers'] = 1 # Number of LSTM stack.\n",
    "config['rnn']['num_class_labels'] = 20\n",
    "config['rnn']['initializer'] = tf.contrib.layers.xavier_initializer()\n",
    "config['rnn']['batch_size'] = config['batch_size']\n",
    "config['rnn']['loss_type'] = 'average' # or 'last_step' # In the case of 'average', average of all time-steps is used instead of the last time-step.\n",
    "\n",
    "config['ip_queue_capacity'] = config['batch_size']*50\n",
    "config['ip_num_read_threads'] = 1\n",
    "\n",
    "config['test_data_dir'] = \"/home/nico/git/uieproject/data/test\"\n",
    "config['test_file_format'] = \"dataTest_%d.tfrecords\"\n",
    "config['test_file_ids'] = list(range(1,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating /home/nico/git/uieproject/sample_code/runs/1497291060/model-10710\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tfRecord input files.\n",
    "test_filenames = [os.path.join(config['test_data_dir'], config['test_file_format'] % i) for i in config['test_file_ids']]\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "test_batch_samples_op, test_batch_ids_op, test_batch_seq_len_op = input_pipeline(test_filenames, config, name='test_input_pipeline', shuffle=False, mode=\"inference\")\n",
    "\n",
    "sess = tf.Session()\n",
    "init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "# Test graph.\n",
    "with tf.name_scope(\"Inference\"):\n",
    "    # Create model\n",
    "    inferCnnModel = CNNModel(config=config['cnn'],\n",
    "                                input_op=test_batch_samples_op, \n",
    "                                mode='inference')\n",
    "    infer_cnn_representations = inferCnnModel.build_graph()\n",
    "    \n",
    "    inferModel = RNNModel(config=config['rnn'], \n",
    "                            input_op=infer_cnn_representations, \n",
    "                            target_op=None, \n",
    "                            seq_len_op=test_batch_seq_len_op,\n",
    "                            mode=\"inference\")\n",
    "    inferModel.build_graph()\n",
    "    \n",
    "# Restore computation graph.\n",
    "saver = tf.train.Saver()\n",
    "# Restore variables.\n",
    "checkpoint_path = config['checkpoint_id']\n",
    "if checkpoint_path is None:\n",
    "    checkpoint_path = tf.train.latest_checkpoint(config['model_dir'])\n",
    "print(\"Evaluating \" + checkpoint_path)\n",
    "saver.restore(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop\n",
    "test_predictions = []\n",
    "test_sample_ids = []\n",
    "try:\n",
    "    while not coord.should_stop():\n",
    "        # Get predicted labels and sample ids for submission csv.\n",
    "        [predictions, sample_ids] = sess.run([inferModel.predictions, test_batch_ids_op], feed_dict={})\n",
    "        test_predictions.extend(predictions)\n",
    "        test_sample_ids.extend(sample_ids)\n",
    "\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Done.')\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()   \n",
    "\n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)\n",
    "\n",
    "# Now you have your predictions. Do whatever you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2174\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "final_predictions = [x+1 for x in test_predictions]\n",
    "\n",
    "submission = np.zeros((config['num_test_samples'], 2))\n",
    "\n",
    "for i in range(0, config['num_test_samples']):\n",
    "    submission[i, 0] = i + 1\n",
    "    submission[i, 1] = final_predictions[i]\n",
    "\n",
    "with open('submission.csv', 'wb') as f:\n",
    "    f.write(b\"Id,Prediction\\n\")\n",
    "    np.savetxt(f,submission,fmt='%i', delimiter=',')\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

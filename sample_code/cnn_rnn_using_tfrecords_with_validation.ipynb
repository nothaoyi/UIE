{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "An example of TFRecord data loading, preprocessing and RNN training.\n",
    "    - Creates threads to read TFRecord files from disk, decode and preprocess.\n",
    "    - Crops and resizes the RGB frames, i.e., images, (32x32) and flatten: 1024 dimensional representation vector.\n",
    "    - Builds recurrent 2-layer LSTM model\n",
    "    - Trains the model on flattened image vectors.\n",
    "\n",
    "You can use 2D CNN for representation learning on images or 3D volumetric CNN on multiple frames. You should find out how to stack CNN and RNN networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(imgs, title=None):\n",
    "    # make sure input is a list\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    \n",
    "    plt.figure()\n",
    "        \n",
    "    for i in range(len(imgs)):\n",
    "        if(title is not None): \n",
    "            plt.suptitle(title)\n",
    "        plt.subplot(1, len(imgs), i+1)\n",
    "        plt.axis(\"off\")\n",
    "        # fix channels for rgb\n",
    "        if len(imgs[i].shape) > 2:\n",
    "            plt.imshow(imgs[i][:,:,[2,1,0]])\n",
    "        else:\n",
    "            plt.imshow(imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/nico/git/uieproject/sample_code/runs/LR(0.001)-LRType(exponential)-RNNLayers(3)-1497392040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "# Get from dataset.\n",
    "config['num_test_samples'] = 2174\n",
    "config['num_validation_samples'] = 1765\n",
    "config['num_training_samples'] = 5722\n",
    "\n",
    "config['batch_size'] = 16\n",
    "config['learning_rate'] = 1e-3\n",
    "# Learning rate is annealed exponentally in 'exponential' case. Don't forget to change annealing configuration in the code.\n",
    "#config['learning_rate_type'] = 'fixed' #'exponential'\n",
    "config['learning_rate_type'] = 'exponential'\n",
    "\n",
    "config['num_steps_per_epoch'] = int(config['num_training_samples']/config['batch_size'])\n",
    "\n",
    "config['num_epochs'] = 1000\n",
    "config['evaluate_every_step'] = config['num_steps_per_epoch']*5\n",
    "config['checkpoint_every_step'] = config['num_steps_per_epoch']*30\n",
    "config['num_validation_steps'] = int(config['num_validation_samples']/config['batch_size'])\n",
    "config['print_every_step'] = config['num_steps_per_epoch']\n",
    "config['log_dir'] = './runs/'\n",
    "\n",
    "config['img_height'] = 80\n",
    "config['img_width'] = 80\n",
    "config['img_num_channels'] = 3\n",
    "config['skeleton_size'] = 180\n",
    "\n",
    "# CNN model parameters\n",
    "config['cnn'] = {}\n",
    "config['cnn']['cnn_filters'] = [16,32,64,128] # Number of filters for every convolutional layer.\n",
    "config['cnn']['num_hidden_units'] = 512 # Number of output units, i.e. representation size.\n",
    "config['cnn']['dropout_rate'] = 0.5\n",
    "config['cnn']['initializer'] = tf.contrib.layers.xavier_initializer()\n",
    "# RNN model parameters\n",
    "config['rnn'] = {}\n",
    "config['rnn']['num_hidden_units'] = 512 # Number of units in an LSTM cell.\n",
    "config['rnn']['num_layers'] = 3 # Number of LSTM stack.\n",
    "config['rnn']['num_class_labels'] = 20\n",
    "config['rnn']['initializer'] = tf.contrib.layers.xavier_initializer()\n",
    "config['rnn']['batch_size'] = config['batch_size']\n",
    "config['rnn']['loss_type'] = 'average' # or 'last_step' # In the case of 'average', average of all time-steps is used instead of the last time-step.\n",
    "\n",
    "config['ip_queue_capacity'] = config['batch_size']*50\n",
    "config['ip_num_read_threads'] = 6\n",
    "\n",
    "config['train_data_dir'] = \"/home/nico/git/uieproject/data/train\"\n",
    "config['train_file_format'] = \"dataTrain_%d.tfrecords\"\n",
    "config['train_file_ids'] = list(range(1,41))\n",
    "config['valid_data_dir'] = \"/home/nico/git/uieproject/data/validation\"\n",
    "config['valid_file_format'] = \"dataValidation_%d.tfrecords\"\n",
    "config['valid_file_ids'] = list(range(1,16))\n",
    "\n",
    "\n",
    "# Create a unique output directory for this experiment.\n",
    "timestamp = str(int(time.time()))\n",
    "model_name = 'LR(%.3f)-LRType(%s)-RNNLayers(%i)-%s' % (config['learning_rate'], config['learning_rate_type'], config['rnn']['num_layers'], timestamp)\n",
    "config['model_dir'] = os.path.abspath(os.path.join(config['log_dir'], model_name))\n",
    "print(\"Writing to {}\\n\".format(config['model_dir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_decode_sequence(filename_queue, config):\n",
    "    # Create a TFRecordReader.\n",
    "    readerOptions = tf.python_io.TFRecordOptions(compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    reader = tf.TFRecordReader(options=readerOptions)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    \n",
    "    # Read one sequence sample.\n",
    "    # The training and validation files contains the following fields:\n",
    "    # - label: label of the sequence which take values between 1 and 20.\n",
    "    # - length: length of the sequence, i.e., number of frames.\n",
    "    # - depth: sequence of depth images. [length x height x width x numChannels]\n",
    "    # - rgb: sequence of rgb images. [length x height x width x numChannels]\n",
    "    # - segmentation: sequence of segmentation maskes. [length x height x width x numChannels]\n",
    "    # - skeleton: sequence of flattened skeleton joint positions. [length x numJoints]\n",
    "    #\n",
    "    # The test files doesn't contain \"label\" field.\n",
    "    # [height, width, numChannels] = [80, 80, 3]\n",
    "    with tf.name_scope(\"TFRecordDecoding\"):\n",
    "        context_encoded, sequence_encoded = tf.parse_single_sequence_example(\n",
    "                serialized_example,\n",
    "                # \"label\" and \"lenght\" are encoded as context features. \n",
    "                context_features={\n",
    "                    \"label\": tf.FixedLenFeature([], dtype=tf.int64),\n",
    "                    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "                },\n",
    "                # \"depth\", \"rgb\", \"segmentation\", \"skeleton\" are encoded as sequence features.\n",
    "                sequence_features={\n",
    "                    \"depth\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"rgb\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"segmentation\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"skeleton\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                })\n",
    "\n",
    "        \n",
    "        # Fetch required data fields. \n",
    "        # TODO: Customize for your design. Assume that only the RGB images are used for now.\n",
    "        # Decode the serialized RGB images.    \n",
    "        seq_rgb = tf.decode_raw(sequence_encoded['rgb'], tf.uint8)\n",
    "        seq_seg = tf.decode_raw(sequence_encoded['segmentation'], tf.uint8)\n",
    "        seq_dep = tf.decode_raw(sequence_encoded['depth'], tf.uint8)\n",
    "        \n",
    "        # Output dimensionality: [seq_len, height, width, numChannels]\n",
    "        \n",
    "        # tf.map_fn applies the preprocessing function to every image in the sequence, i.e., frame.\n",
    "        seq_rgb = tf.map_fn(lambda x: preprocessing_op(x, config),\n",
    "                                elems=(seq_rgb,seq_seg,seq_dep),\n",
    "                                dtype=tf.float32,\n",
    "                                back_prop=False)\n",
    "        \n",
    "        seq_label = context_encoded['label']\n",
    "        \n",
    "        # Tensorflow requires the labels start from 0. Before you create submission csv, \n",
    "        # increment the predictions by 1.\n",
    "        seq_label = seq_label - 1\n",
    "        seq_len = tf.to_int32(context_encoded['length'])\n",
    "        \n",
    "        \"\"\"\n",
    "        # Use skeleton only.\n",
    "        seq_skeleton = tf.decode_raw(sequence_encoded['skeleton'], tf.float32)\n",
    "        # Normalize skeleton so that every pose is a unit length vector.\n",
    "        seq_skeleton = tf.nn.l2_normalize(seq_skeleton, dim=1)\n",
    "        seq_skeleton.set_shape([None, config['skeleton_size']])\n",
    "        \n",
    "        seq_len = tf.to_int32(context_encoded['length'])\n",
    "        seq_label = context_encoded['label']\n",
    "        # Tensorflow requires the labels start from 0. Before you create submission csv, \n",
    "        # increment the predictions by 1.\n",
    "        seq_label = seq_label - 1\n",
    "        \"\"\"\n",
    "        \n",
    "        return [seq_rgb, seq_label, seq_len]\n",
    "    \n",
    "\n",
    "def input_pipeline(filenames, config, name='input_pipeline', shuffle=True):\n",
    "    with tf.name_scope(name):\n",
    "        # Create a queue of TFRecord input files.\n",
    "        filename_queue = tf.train.string_input_producer(filenames, num_epochs=config['num_epochs'], shuffle=shuffle)\n",
    "        # Read the data from TFRecord files, decode and create a list of data samples by using threads.\n",
    "        sample_list = [read_and_decode_sequence(filename_queue, config) for _ in range(config['ip_num_read_threads'])]\n",
    "        # Create batches.\n",
    "        # Since the data consists of variable-length sequences, allow padding by setting dynamic_pad parameter.\n",
    "        # \"batch_join\" creates batches of samples and pads the sequences w.r.t the max-length sequence in the batch.\n",
    "        # Hence, the padded sequence length can be different for different batches.\n",
    "        batch_rgb, batch_labels, batch_lens = tf.train.batch_join(sample_list,\n",
    "                                                    batch_size=config['batch_size'],\n",
    "                                                    capacity=config['ip_queue_capacity'],\n",
    "                                                    enqueue_many=False,\n",
    "                                                    dynamic_pad=True,\n",
    "                                                    allow_smaller_final_batch = False,\n",
    "                                                    name=\"batch_join_and_pad\")\n",
    "\n",
    "        return batch_rgb, batch_labels, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_op(frame, config):\n",
    "    \"\"\"\n",
    "    Creates preprocessing operations that are going to be applied on a single frame.\n",
    "    \n",
    "    TODO: Customize for your needs.\n",
    "    You can do any preprocessing (masking, normalization/scaling of inputs, augmentation, etc.) by using tensorflow operations.\n",
    "    Built-in image operations: https://www.tensorflow.org/api_docs/python/tf/image \n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"preprocessing\"):\n",
    "        rgb = frame[0]\n",
    "        seg = frame[1]\n",
    "        dep = frame[2]\n",
    "        \n",
    "        # Reshape serialized image.\n",
    "        rgb = tf.reshape(rgb, (config['img_height'], \n",
    "                               config['img_width'], \n",
    "                               config['img_num_channels'])\n",
    "                        )\n",
    "        \n",
    "        seg = tf.reshape(seg, (config['img_height'], \n",
    "                               config['img_width'], \n",
    "                               config['img_num_channels'])\n",
    "                        )\n",
    "        \n",
    "        dep = tf.reshape(dep, (config['img_height'], \n",
    "                               config['img_width'], \n",
    "                               1)\n",
    "                        )\n",
    "        \n",
    "        # Convert from RGB to grayscale.\n",
    "        rgb = tf.image.rgb_to_grayscale(rgb)\n",
    "        seg = tf.image.rgb_to_grayscale(seg)\n",
    "        dep = tf.image.rgb_to_grayscale(dep)\n",
    "        \n",
    "        # Integer to float.\n",
    "        rgb = tf.to_float(rgb)\n",
    "        dep = tf.to_float(dep)\n",
    "        \n",
    "        seg = tf.greater(seg, 150)\n",
    "        \n",
    "        rgb = tf.where(seg, rgb, tf.zeros_like(rgb))\n",
    "        dep = tf.where(seg, dep, tf.zeros_like(rgb))\n",
    "        \n",
    "        # Crop\n",
    "        #image_op = tf.image.resize_image_with_crop_or_pad(image_op, 60, 60)\n",
    "        \n",
    "        # Resize operation requires 4D tensors (i.e., batch of images).\n",
    "        # Reshape the image so that it looks like a batch of one sample: [1,60,60,1]\n",
    "        #image_op = tf.expand_dims(image_op, 0)\n",
    "        # Resize\n",
    "        #image_op = tf.image.resize_bilinear(image_op, np.asarray([32,32]))\n",
    "        # Reshape the image: [32,32,1]\n",
    "        #image_op = tf.squeeze(image_op, 0)\n",
    "        \n",
    "        # Normalize (zero-mean unit-variance) the image locally, i.e., by using statistics of the \n",
    "        # image not the whole data or sequence.\n",
    "        \n",
    "        rgb = tf.image.per_image_standardization(rgb)\n",
    "        dep = tf.image.per_image_standardization(dep)\n",
    "        \n",
    "        rgb = tf.squeeze(rgb)\n",
    "        dep = tf.squeeze(dep)\n",
    "        \n",
    "        # Flatten image\n",
    "        #image_op = tf.reshape(image_op, [-1])\n",
    "    \n",
    "        return tf.stack([rgb, dep], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "class CNNModel():\n",
    "    \"\"\"\n",
    "    Creates training and validation computational graphs.\n",
    "    Note that tf.variable_scope enables sharing the parameters so that both graphs share the parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, input_op, mode):\n",
    "        \"\"\"\n",
    "        Basic setup.\n",
    "        Args:\n",
    "          config: Object containing configuration parameters.\n",
    "        \"\"\"\n",
    "        assert mode in [\"training\", \"validation\"]\n",
    "        self.config = config\n",
    "        self.inputs = input_op\n",
    "        self.mode = mode\n",
    "        self.is_training = self.mode == \"training\"\n",
    "        self.reuse = self.mode == \"validation\"\n",
    "        \n",
    "        \n",
    "    def build_model(self, input_layer):\n",
    "        with tf.variable_scope(\"cnn_model\", reuse=self.reuse, initializer=self.config['initializer']):\n",
    "            # Convolutional Layer #1\n",
    "            # Computes 32 features using a 3x3 filter with ReLU activation.\n",
    "            # Padding is added to preserve width and height.\n",
    "            # Input Tensor Shape: [batch_size, 80, 80, num_channels]\n",
    "            # Output Tensor Shape: [batch_size, 40, 40, num_filter1]\n",
    "            conv1 = tf.layers.conv2d(\n",
    "                inputs=input_layer,\n",
    "                filters=self.config['cnn_filters'][0],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2, padding='same')\n",
    "\n",
    "            # Input Tensor Shape: [batch_size, 40, 40, num_filter1]\n",
    "            # Output Tensor Shape: [batch_size, 20, 20, num_filter2]\n",
    "            conv2 = tf.layers.conv2d(\n",
    "                inputs=pool1,\n",
    "                filters=self.config['cnn_filters'][1],\n",
    "                kernel_size=[5, 5],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2, padding='same')\n",
    "\n",
    "            # Input Tensor Shape: [batch_size, 20, 20, num_filter2]\n",
    "            # Output Tensor Shape: [batch_size, 10, 10, num_filter3]\n",
    "            conv3 = tf.layers.conv2d(\n",
    "                inputs=pool2,\n",
    "                filters=self.config['cnn_filters'][2],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2, padding='same')\n",
    "\n",
    "            # Input Tensor Shape: [batch_size, 10, 10, num_filter3]\n",
    "            # Output Tensor Shape: [batch_size, 5, 5, num_filter4]\n",
    "            conv4 = tf.layers.conv2d(\n",
    "                inputs=pool3,\n",
    "                filters=self.config['cnn_filters'][3],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "            pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2, padding='same')\n",
    "\n",
    "\n",
    "            # Flatten tensor into a batch of vectors\n",
    "            # Input Tensor Shape: [batch_size, 5, 5, num_filter4]\n",
    "            # Output Tensor Shape: [batch_size, 5 * 5 * num_filter4]\n",
    "            conv_flat = tf.reshape(pool4, [-1, 5 * 5 * self.config['cnn_filters'][3]])\n",
    "\n",
    "            # Add dropout operation;\n",
    "            dropout = tf.layers.dropout(inputs=conv_flat, rate=self.config['dropout_rate'], training=self.is_training)\n",
    "\n",
    "            # Dense Layer\n",
    "            # Densely connected layer with <num_hidden_units> neurons\n",
    "            # Input Tensor Shape: [batch_size, 5 * 5 * num_filter4]\n",
    "            # Output Tensor Shape: [batch_size, num_hidden_units]\n",
    "            dense = tf.layers.dense(inputs=dropout, units=self.config['num_hidden_units'],\n",
    "                activation=tf.nn.relu)\n",
    "        \n",
    "            self.cnn_model = dense\n",
    "            return dense\n",
    "            \n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        CNNs accept inputs of shape (batch_size, height, width, num_channels). However, we have inputs of shape\n",
    "        (batch_size, sequence_length, height, width, num_channels) where sequence_length is inferred at run time.\n",
    "        We need to iterate in order to get CNN representations. Similar to python's map function, \"tf.map_fn\" \n",
    "        applies a given function on each entry in the input list. \n",
    "        \"\"\"\n",
    "        # For the first time create a dummy graph and then share the parameters everytime.\n",
    "        if self.is_training:\n",
    "            self.reuse = False\n",
    "            self.build_model(self.inputs[0])\n",
    "            self.reuse = True\n",
    "        \n",
    "        # CNN takes a clip as if it is a batch of samples.\n",
    "        # Have a look at tf.map_fn (https://www.tensorflow.org/api_docs/python/tf/map_fn)\n",
    "        # You can set parallel_iterations or swap_memory in order to make it faster.\n",
    "        # Note that back_prop argument is True in order to enable training of CNN.\n",
    "        self.cnn_representations = tf.map_fn(lambda x: self.build_model(x),\n",
    "                                                elems=self.inputs,\n",
    "                                                dtype=tf.float32,\n",
    "                                                back_prop=True,\n",
    "                                                swap_memory=True,\n",
    "                                                parallel_iterations=2)\n",
    "        \n",
    "        return self.cnn_representations\n",
    "        \n",
    "\n",
    "class RNNModel():\n",
    "    \"\"\"\n",
    "    Creates training and validation computational graphs.\n",
    "    Note that tf.variable_scope enables sharing the parameters so that both graphs share the parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, input_op, target_op, seq_len_op, mode):\n",
    "        \"\"\"\n",
    "        Basic setup.\n",
    "        Args:\n",
    "          config: Object containing configuration parameters.\n",
    "        \"\"\"\n",
    "        assert mode in [\"training\", \"validation\"]\n",
    "        self.config = config\n",
    "        self.inputs = input_op\n",
    "        self.targets = target_op\n",
    "        self.seq_lengths = seq_len_op\n",
    "        self.mode = mode\n",
    "        self.reuse = self.mode == \"validation\"\n",
    "        \n",
    "    def build_rnn_model(self):\n",
    "        with tf.variable_scope('rnn_cell', reuse=self.reuse, initializer=self.config['initializer']):\n",
    "            rnn_cell = tf.contrib.rnn.BasicLSTMCell(num_units=self.config['num_hidden_units'])\n",
    "        with tf.variable_scope('rnn_stack', reuse=self.reuse, initializer=self.config['initializer']):\n",
    "            if self.config['num_layers'] > 1:\n",
    "                rnn_cell = tf.contrib.rnn.MultiRNNCell([rnn_cell for _ in range(self.config['num_layers'])])\n",
    "            self.model_rnn, self.rnn_state = tf.nn.dynamic_rnn(\n",
    "                                            cell=rnn_cell,\n",
    "                                            inputs=self.inputs,\n",
    "                                            dtype = tf.float32,\n",
    "                                            sequence_length=self.seq_lengths,\n",
    "                                            time_major=False,\n",
    "                                            swap_memory=True)\n",
    "            # Fetch output of the last step.\n",
    "            if self.config['loss_type'] == 'last_step':\n",
    "                self.rnn_prediction = tf.gather_nd(self.model_rnn, tf.stack([tf.range(self.config['batch_size']), self.seq_lengths-1], axis=1))\n",
    "            elif self.config['loss_type'] == 'average':\n",
    "                self.rnn_prediction = self.model_rnn\n",
    "            else:\n",
    "                print(\"Invalid loss type\")\n",
    "                raise\n",
    "                \n",
    "    \n",
    "    def build_model(self):\n",
    "        self.build_rnn_model()\n",
    "        # Calculate logits\n",
    "        with tf.variable_scope('logits', reuse=self.reuse, initializer=self.config['initializer']):\n",
    "            self.logits = tf.layers.dense(inputs=self.rnn_prediction, units=self.config['num_class_labels'],\n",
    "                                            kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                            bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            # In the case of average loss, take average of time steps in order to calculate\n",
    "            # final prediction probabilities.\n",
    "            if self.config['loss_type'] == 'average':\n",
    "                self.logits = tf.reduce_mean(self.logits, axis=1)\n",
    "            \n",
    "    def loss(self):\n",
    "        # Loss calculations: cross-entropy\n",
    "        with tf.name_scope(\"cross_entropy_loss\"):\n",
    "            self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.targets))\n",
    "            \n",
    "                # Accuracy calculations.\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # Return list of predictions (useful for making a submission)\n",
    "            self.predictions = tf.argmax(self.logits, 1, name=\"predictions\")\n",
    "            # Return a bool tensor with shape [batch_size] that is true for the\n",
    "            # correct predictions.\n",
    "            self.correct_predictions = tf.equal(tf.argmax(self.logits, 1), self.targets)\n",
    "            # Number of correct predictions in order to calculate average accuracy afterwards.\n",
    "            self.num_correct_predictions = tf.reduce_sum(tf.cast(self.correct_predictions, tf.int32))\n",
    "            # Calculate the accuracy per minibatch.\n",
    "            self.batch_accuracy = tf.reduce_mean(tf.cast(self.correct_predictions, tf.float32))\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self.build_model()\n",
    "        self.loss()\n",
    "        self.num_parameters()\n",
    "        \n",
    "        return self.logits, self.loss, self.batch_accuracy, self.predictions\n",
    "    \n",
    "    def num_parameters(self):\n",
    "        self.num_parameters = 0\n",
    "        #iterating over all variables\n",
    "        for variable in tf.trainable_variables():\n",
    "            local_parameters=1\n",
    "            shape = variable.get_shape()  #getting shape of a variable\n",
    "            for i in shape:\n",
    "                local_parameters*=i.value  #mutiplying dimension values\n",
    "            self.num_parameters+=local_parameters\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, ?, 80, 80, 2)\n",
      "\n",
      "# of parameters: 8052260\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tfRecord input files.\n",
    "train_filenames = [os.path.join(config['train_data_dir'], config['train_file_format'] % i) for i in config['train_file_ids']]\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "train_batch_samples_op, train_batch_labels_op, train_batch_seq_len_op = input_pipeline(train_filenames, config, name='training_input_pipeline')\n",
    "\n",
    "print(train_batch_samples_op.shape)\n",
    "\n",
    "# Create a list of tfRecord input files.\n",
    "valid_filenames = [os.path.join(config['valid_data_dir'], config['valid_file_format'] % i) for i in config['valid_file_ids']]\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "valid_batch_samples_op, valid_batch_labels_op, valid_batch_seq_len_op = input_pipeline(valid_filenames, config, name='validation_input_pipeline', shuffle=False)\n",
    "\n",
    "# Create placeholders for training and monitoring variables.\n",
    "loss_avg_op = tf.placeholder(tf.float32, name=\"loss_avg\")\n",
    "accuracy_avg_op = tf.placeholder(tf.float32, name=\"accuracy_avg\")\n",
    "\n",
    "# Generate a variable to contain a counter for the global training step.\n",
    "# Note that it is useful if you save/restore your network.\n",
    "global_step = tf.Variable(1, name='global_step', trainable=False)\n",
    "\n",
    "# Create seperate graphs for training and validation.\n",
    "# Training graph\n",
    "# Note that our model is optimized by using the training graph.\n",
    "with tf.name_scope(\"Training\"):\n",
    "    # Create model\n",
    "    cnnModel = CNNModel(config=config['cnn'],\n",
    "                        input_op=train_batch_samples_op, \n",
    "                        mode='training')\n",
    "    cnn_representations = cnnModel.build_graph()\n",
    "    \n",
    "    trainModel = RNNModel(config=config['rnn'], \n",
    "                            input_op=cnn_representations, \n",
    "                            target_op=train_batch_labels_op, \n",
    "                            seq_len_op=train_batch_seq_len_op,\n",
    "                            mode=\"training\")\n",
    "    trainModel.build_graph()\n",
    "    print(\"\\n# of parameters: %s\" % trainModel.num_parameters)\n",
    "    \n",
    "    # Optimization routine.\n",
    "    # Learning rate is decayed in time. This enables our model using higher learning rates in the beginning.\n",
    "    # In time the learning rate is decayed so that gradients don't explode and training staurates.\n",
    "    # If you observe slow training, feel free to modify decay_steps and decay_rate arguments.\n",
    "    if config['learning_rate_type'] == 'exponential':\n",
    "        learning_rate = tf.train.exponential_decay(config['learning_rate'], \n",
    "                                                   global_step=global_step,\n",
    "                                                   decay_steps=1000, \n",
    "                                                   decay_rate=0.97,\n",
    "                                                   staircase=False)\n",
    "    elif config['learning_rate_type'] == 'fixed':\n",
    "        learning_rate = config['learning_rate']\n",
    "    else:\n",
    "        print(\"Invalid learning rate type\")\n",
    "        raise\n",
    "        \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(trainModel.loss, global_step=global_step)\n",
    "\n",
    "# Validation graph.\n",
    "with tf.name_scope(\"Evaluation\"):\n",
    "    # Create model\n",
    "    validCnnModel = CNNModel(config=config['cnn'],\n",
    "                                input_op=valid_batch_samples_op, \n",
    "                                mode='validation')\n",
    "    valid_cnn_representations = validCnnModel.build_graph()\n",
    "    \n",
    "    validModel = RNNModel(config=config['rnn'], \n",
    "                            input_op=valid_cnn_representations, \n",
    "                            target_op=valid_batch_labels_op, \n",
    "                            seq_len_op=valid_batch_seq_len_op,\n",
    "                            mode=\"validation\")\n",
    "    validModel.build_graph()\n",
    "\n",
    "    \n",
    "# Create summary ops for monitoring the training.\n",
    "# Each summary op annotates a node in the computational graph and collects\n",
    "# data data from it.\n",
    "summary_train_loss = tf.summary.scalar('loss', trainModel.loss)\n",
    "summary_train_acc = tf.summary.scalar('accuracy_training', trainModel.batch_accuracy)\n",
    "summary_avg_accuracy = tf.summary.scalar('accuracy_avg', accuracy_avg_op)\n",
    "summary_avg_loss = tf.summary.scalar('loss_avg', loss_avg_op)\n",
    "summary_learning_rate = tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "# Group summaries.\n",
    "# summaries_training is used during training and reported after every step.\n",
    "summaries_training = tf.summary.merge([summary_train_loss, summary_train_acc, summary_learning_rate])\n",
    "# summaries_evaluation is used by both trainig and validation in order to report the performance on the dataset.\n",
    "summaries_evaluation = tf.summary.merge([summary_avg_accuracy, summary_avg_loss])\n",
    "    \n",
    "#Create session object\n",
    "sess = tf.Session()\n",
    "# Add the ops to initialize variables.\n",
    "init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "# Actually intialize the variables\n",
    "sess.run(init_op)\n",
    "\n",
    "# Register summary ops.\n",
    "train_summary_dir = os.path.join(config['model_dir'], \"summary\", \"train\")\n",
    "train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "valid_summary_dir = os.path.join(config['model_dir'], \"summary\", \"validation\")\n",
    "valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "\n",
    "# Create a saver for writing training checkpoints.\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "# Define counters in order to accumulate measurements.\n",
    "counter_correct_predictions_training = 0.0\n",
    "counter_loss_training = 0.0\n",
    "counter_correct_predictions_validation = 0.0\n",
    "counter_loss_validation = 0.0\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "try:\n",
    "    while not coord.should_stop():\n",
    "        step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "        if (step%config['checkpoint_every_step']) == 0:\n",
    "            ckpt_save_path = saver.save(sess, os.path.join(config['model_dir'], 'model'), global_step)\n",
    "            print(\"Model saved in file: %s\" % ckpt_save_path)\n",
    "            \n",
    "        # Run the optimizer to update weights.\n",
    "        # Note that \"train_op\" is responsible from updating network weights.\n",
    "        # Only the operations that are fed are evaluated.\n",
    "        # Run the optimizer to update weights.\n",
    "        train_summary, num_correct_predictions, loss, _ = sess.run([summaries_training, \n",
    "                                                                      trainModel.num_correct_predictions, \n",
    "                                                                      trainModel.loss, \n",
    "                                                                      train_op], \n",
    "                                                                      feed_dict={})\n",
    "        # Update counters.\n",
    "        counter_correct_predictions_training += num_correct_predictions\n",
    "        counter_loss_training += loss\n",
    "        # Write summary data.\n",
    "        train_summary_writer.add_summary(train_summary, step)\n",
    "        \n",
    "        # Report training performance\n",
    "        if (step%config['print_every_step']) == 0:\n",
    "            accuracy_avg = counter_correct_predictions_training / (config['batch_size']*config['print_every_step'])\n",
    "            loss_avg = counter_loss_training / (config['print_every_step'])\n",
    "            summary_report = sess.run(summaries_evaluation, feed_dict={accuracy_avg_op:accuracy_avg, loss_avg_op:loss_avg})\n",
    "            train_summary_writer.add_summary(summary_report, step)\n",
    "            print(\"[%d/%d] [Training] Accuracy: %.3f, Loss: %.3f\" % (step/config['num_steps_per_epoch'], \n",
    "                                                                     step, \n",
    "                                                                     accuracy_avg, \n",
    "                                                                     loss_avg))\n",
    "            \n",
    "            counter_correct_predictions_training = 0.0\n",
    "            counter_loss_training= 0.0\n",
    "        \n",
    "        if (step%config['evaluate_every_step']) == 0:\n",
    "            # It is possible to create only one input pipelene queue. Hence, we create a validation queue \n",
    "            # in the begining for multiple epochs and control it via a foor loop.\n",
    "            # Note that we only approximate 1 validation epoch (validation doesn't have to be accurate.)\n",
    "            # In other words, number of unique validation samples may differ everytime.\n",
    "            for eval_step in range(config['num_validation_steps']):\n",
    "                # Calculate average validation accuracy.\n",
    "                num_correct_predictions, loss = sess.run([validModel.num_correct_predictions, \n",
    "                                                          validModel.loss],\n",
    "                                                         feed_dict={})\n",
    "                # Update counters.\n",
    "                counter_correct_predictions_validation += num_correct_predictions\n",
    "                counter_loss_validation += loss\n",
    "            \n",
    "            # Report validation performance\n",
    "            accuracy_avg = counter_correct_predictions_validation / (config['batch_size']*config['num_validation_steps'])\n",
    "            loss_avg = counter_loss_validation / (config['num_validation_steps'])\n",
    "            summary_report = sess.run(summaries_evaluation, feed_dict={accuracy_avg_op:accuracy_avg, loss_avg_op:loss_avg})\n",
    "            valid_summary_writer.add_summary(summary_report, step)\n",
    "            print(\"[%d/%d] [Validation] Accuracy: %.3f, Loss: %.3f\" % (step/config['num_steps_per_epoch'], \n",
    "                                                                       step, \n",
    "                                                                       accuracy_avg, \n",
    "                                                                       loss_avg))\n",
    "            \n",
    "            counter_correct_predictions_validation = 0.0\n",
    "            counter_loss_validation= 0.0\n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Model is trained for %d epochs, %d steps.' % (config['num_epochs'], step))\n",
    "    print('Done.')\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()\n",
    "\n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)\n",
    "\n",
    "ckpt_save_path = saver.save(sess, os.path.join(config['model_dir'], 'model'), global_step)\n",
    "print(\"Model saved in file: %s\" % ckpt_save_path)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, ?, 80, 80, 2)\n",
      "# Samples: 16\n",
      "Sequence lengths: [63 88 60 84 82 76 98 71 63 54 59 58 61 81 59 70]\n",
      "Sequence labels: [ 3 11 19 11 18 14 13  4 13 19 10  9  1 16  2  1]\n",
      "Sequence type: <class 'numpy.ndarray'>\n",
      "Sequence shape: (16, 98, 80, 80, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbe065ad68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIvABgwh8wCACHzCIwAcM+j/AqZimGECCmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbbc6fea20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDyhE4AMKEfiAQv8HlKDuUMiRcHgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbbc6e24e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/T+CIt1puyX60QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbbc6ac198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb9c7bf470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "A simple sanity check for input pipeline:\n",
    "'''\n",
    "def test_input_loader():\n",
    "    config = {}\n",
    "    config['img_height'] = 80\n",
    "    config['img_width'] = 80\n",
    "    config['img_num_channels'] = 3\n",
    "    config['num_epochs'] = 10\n",
    "    config['batch_size'] = 16\n",
    "    # Capacity of the queue which contains the samples read by data readers.\n",
    "    # Make sure that it has enough capacity.\n",
    "    config['ip_queue_capacity'] = config['batch_size']*10  \n",
    "    config['ip_num_read_threads'] = 6\n",
    "    # Directory of the data.\n",
    "    config['data_dir'] = \"/home/nico/git/uieproject/data/train\"\n",
    "    # File naming\n",
    "    config['file_format'] = \"dataTrain_%d.tfrecords\"\n",
    "    # File IDs to be used\n",
    "    config['file_ids'] = list(range(1,10))\n",
    "    \n",
    "    # Create a list of TFRecord input files.\n",
    "    filenames = [os.path.join(config['data_dir'], config['file_format'] % i) for i in config['file_ids']]\n",
    "\n",
    "    # Create data loading operators. This will be represented as a node in the computational graph.\n",
    "    batch_samples_op, batch_labels_op, batch_seq_len_op = input_pipeline(filenames, config)\n",
    "    \n",
    "    print(batch_samples_op.shape)\n",
    "    # TODO: batch_samples_op, batch_labels_op and batch_seq_len_op are like input placeholders. You can directly \n",
    "    # feed them to your model.\n",
    "\n",
    "    # Create tensorflow session and initialize the variables (if any).\n",
    "    sess = tf.Session()\n",
    "    init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    # Create threads to prefetch the data.\n",
    "    # https://www.tensorflow.org/programmers_guide/reading_data#creating_threads_to_prefetch_using_queuerunner_objects\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    batch_samples, batch_labels, batch_seq_len = sess.run([batch_samples_op, batch_labels_op, batch_seq_len_op])\n",
    "    \n",
    "    # Print \n",
    "    print(\"# Samples: \" + str(len(batch_samples)))\n",
    "    print(\"Sequence lengths: \" + str(batch_seq_len))\n",
    "    print(\"Sequence labels: \" + str(batch_labels))\n",
    "    \n",
    "    # Note that the second dimension will give maximum-length in the batch, i.e., the padded sequence length.\n",
    "    print(\"Sequence type: \" + str(type(batch_samples)))\n",
    "    print(\"Sequence shape: \" + str(batch_samples.shape))\n",
    "\n",
    "    # plot some\n",
    "    plot(batch_samples[0][10][...,1].squeeze())\n",
    "    plot(batch_samples[1][20][...,1].squeeze())\n",
    "    plot(batch_samples[2][30][...,1].squeeze())\n",
    "    plot(batch_samples[3][15][...,1].squeeze())\n",
    "    plot(batch_samples[4][50][...,1].squeeze())\n",
    "    \n",
    "test_input_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "# Get from dataset.\n",
    "config['num_test_samples'] = 2174\n",
    "config['num_validation_samples'] = 1765\n",
    "config['num_training_samples'] = 5722\n",
    "\n",
    "config['batch_size'] = 16\n",
    "config['learning_rate'] = 1e-3\n",
    "# Learning rate is annealed exponentally in 'exponential' case. Don't forget to change annealing configuration in the code.\n",
    "#config['learning_rate_type'] = 'fixed' #'exponential'\n",
    "config['learning_rate_type'] = 'exponential'\n",
    "\n",
    "config['num_steps_per_epoch'] = int(config['num_training_samples']/config['batch_size'])\n",
    "\n",
    "config['num_epochs'] = 1000\n",
    "config['evaluate_every_step'] = config['num_steps_per_epoch']*5\n",
    "config['checkpoint_every_step'] = config['num_steps_per_epoch']*30\n",
    "config['num_validation_steps'] = int(config['num_validation_samples']/config['batch_size'])\n",
    "config['print_every_step'] = config['num_steps_per_epoch']\n",
    "config['log_dir'] = './sample_code/runs/'\n",
    "\n",
    "config['img_height'] = 80\n",
    "config['img_width'] = 80\n",
    "config['img_num_channels'] = 3\n",
    "config['skeleton_size'] = 180\n",
    "\n",
    "# CNN model parameters\n",
    "config['cnn'] = {}\n",
    "config['cnn']['cnn_filters'] = [16,32,64,128] # Number of filters for every convolutional layer.\n",
    "config['cnn']['num_hidden_units'] = 512 # Number of output units, i.e. representation size.\n",
    "config['cnn']['dropout_rate'] = 0.5\n",
    "config['cnn']['initializer'] = tf.contrib.layers.xavier_initializer()\n",
    "# RNN model parameters\n",
    "config['rnn'] = {}\n",
    "config['rnn']['num_hidden_units'] = 512 # Number of units in an LSTM cell.\n",
    "config['rnn']['num_layers'] = 1 # Number of LSTM stack.\n",
    "config['rnn']['num_class_labels'] = 20\n",
    "config['rnn']['initializer'] = tf.contrib.layers.xavier_initializer()\n",
    "config['rnn']['batch_size'] = config['batch_size']\n",
    "config['rnn']['loss_type'] = 'average' # or 'last_step' # In the case of 'average', average of all time-steps is used instead of the last time-step.\n",
    "\n",
    "config['ip_queue_capacity'] = config['batch_size']*50\n",
    "config['ip_num_read_threads'] = 6\n",
    "\n",
    "config['train_data_dir'] = \"/home/nico/git/uieproject/data/train\"\n",
    "config['train_file_format'] = \"dataTrain_%d.tfrecords\"\n",
    "config['train_file_ids'] = list(range(1,41))\n",
    "config['valid_data_dir'] = \"/home/nico/git/uieproject/data/validation\"\n",
    "config['valid_file_format'] = \"dataValidation_%d.tfrecords\"\n",
    "config['valid_file_ids'] = list(range(1,16))\n",
    "\n",
    "\n",
    "# Create a unique output directory for this experiment.\n",
    "timestamp = str(int(time.time()))\n",
    "model_name = 'LR(%.3f)-LRType(%s)-%s' % (config['learning_rate'], config['learning_rate_type'], timestamp)\n",
    "config['model_dir'] = os.path.abspath(os.path.join(config['log_dir'], model_name))\n",
    "print(\"Writing to {}\\n\".format(config['model_dir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_op(image_op, config):\n",
    "    \"\"\"\n",
    "    Creates preprocessing operations that are going to be applied on a single frame.\n",
    "    \n",
    "    TODO: Customize for your needs.\n",
    "    You can do any preprocessing (masking, normalization/scaling of inputs, augmentation, etc.) by using tensorflow operations.\n",
    "    Built-in image operations: https://www.tensorflow.org/api_docs/python/tf/image \n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"preprocessing\"):\n",
    "        # mask images\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Reshape serialized image.\n",
    "        image_op = tf.reshape(image_op, (config['img_height'], \n",
    "                                         config['img_width'], \n",
    "                                         config['img_num_channels'])\n",
    "                          )\n",
    "        # Convert from RGB to grayscale.\n",
    "        image_op = tf.image.rgb_to_grayscale(image_op)\n",
    "        \n",
    "        # Integer to float.\n",
    "        image_op = tf.to_float(image_op)\n",
    "        # Crop\n",
    "        #image_op = tf.image.resize_image_with_crop_or_pad(image_op, 60, 60)\n",
    "        \n",
    "        # Resize operation requires 4D tensors (i.e., batch of images).\n",
    "        # Reshape the image so that it looks like a batch of one sample: [1,60,60,1]\n",
    "        #image_op = tf.expand_dims(image_op, 0)\n",
    "        # Resize\n",
    "        #image_op = tf.image.resize_bilinear(image_op, np.asarray([32,32]))\n",
    "        # Reshape the image: [32,32,1]\n",
    "        #image_op = tf.squeeze(image_op, 0)\n",
    "        \n",
    "        # Normalize (zero-mean unit-variance) the image locally, i.e., by using statistics of the \n",
    "        # image not the whole data or sequence. \n",
    "        image_op = tf.image.per_image_standardization(image_op)\n",
    "        \n",
    "        # Flatten image\n",
    "        #image_op = tf.reshape(image_op, [-1])\n",
    "    \n",
    "        return image_op\n",
    "\n",
    "def read_and_decode_sequence(filename_queue, config):\n",
    "    # Create a TFRecordReader.\n",
    "    readerOptions = tf.python_io.TFRecordOptions(compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    reader = tf.TFRecordReader(options=readerOptions)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    \n",
    "    # Read one sequence sample.\n",
    "    # The training and validation files contains the following fields:\n",
    "    # - label: label of the sequence which take values between 1 and 20.\n",
    "    # - length: length of the sequence, i.e., number of frames.\n",
    "    # - depth: sequence of depth images. [length x height x width x numChannels]\n",
    "    # - rgb: sequence of rgb images. [length x height x width x numChannels]\n",
    "    # - segmentation: sequence of segmentation maskes. [length x height x width x numChannels]\n",
    "    # - skeleton: sequence of flattened skeleton joint positions. [length x numJoints]\n",
    "    #\n",
    "    # The test files doesn't contain \"label\" field.\n",
    "    # [height, width, numChannels] = [80, 80, 3]\n",
    "    with tf.name_scope(\"TFRecordDecoding\"):\n",
    "        context_encoded, sequence_encoded = tf.parse_single_sequence_example(\n",
    "                serialized_example,\n",
    "                # \"label\" and \"lenght\" are encoded as context features. \n",
    "                context_features={\n",
    "                    \"label\": tf.FixedLenFeature([], dtype=tf.int64),\n",
    "                    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "                },\n",
    "                # \"depth\", \"rgb\", \"segmentation\", \"skeleton\" are encoded as sequence features.\n",
    "                sequence_features={\n",
    "                    \"depth\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"rgb\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"segmentation\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"skeleton\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                })\n",
    "\n",
    "        \n",
    "        # Fetch required data fields. \n",
    "        # TODO: Customize for your design. Assume that only the RGB images are used for now.\n",
    "        # Decode the serialized RGB images.    \n",
    "        seq_rgb = tf.decode_raw(sequence_encoded['rgb'], tf.uint8)\n",
    "        seq_label = context_encoded['label']\n",
    "        # Tensorflow requires the labels start from 0. Before you create submission csv, \n",
    "        # increment the predictions by 1.\n",
    "        seq_label = seq_label - 1\n",
    "        seq_len = tf.to_int32(context_encoded['length'])\n",
    "        # Output dimnesionality: [seq_len, height, width, numChannels]\n",
    "        # tf.map_fn applies the preprocessing function on every image in the sequence, i.e., frame.\n",
    "        seq_rgb = tf.map_fn(lambda x: preprocessing_op(x, config),\n",
    "                                elems=seq_rgb,\n",
    "                                dtype=tf.float32,\n",
    "                                back_prop=False)     \n",
    "        \"\"\"\n",
    "        # Use skeleton only.\n",
    "        seq_skeleton = tf.decode_raw(sequence_encoded['skeleton'], tf.float32)\n",
    "        # Normalize skeleton so that every pose is a unit length vector.\n",
    "        seq_skeleton = tf.nn.l2_normalize(seq_skeleton, dim=1)\n",
    "        seq_skeleton.set_shape([None, config['skeleton_size']])\n",
    "        \n",
    "        seq_len = tf.to_int32(context_encoded['length'])\n",
    "        seq_label = context_encoded['label']\n",
    "        # Tensorflow requires the labels start from 0. Before you create submission csv, \n",
    "        # increment the predictions by 1.\n",
    "        seq_label = seq_label - 1\n",
    "        \"\"\"\n",
    "        \n",
    "        return [seq_rgb, seq_label, seq_len]\n",
    "    \n",
    "\n",
    "def input_pipeline(filenames, config, name='input_pipeline', shuffle=True):\n",
    "    with tf.name_scope(name):\n",
    "        # Create a queue of TFRecord input files.\n",
    "        filename_queue = tf.train.string_input_producer(filenames, num_epochs=config['num_epochs'], shuffle=shuffle)\n",
    "        # Read the data from TFRecord files, decode and create a list of data samples by using threads.\n",
    "        sample_list = [read_and_decode_sequence(filename_queue, config) for _ in range(config['ip_num_read_threads'])]\n",
    "        # Create batches.\n",
    "        # Since the data consists of variable-length sequences, allow padding by setting dynamic_pad parameter.\n",
    "        # \"batch_join\" creates batches of samples and pads the sequences w.r.t the max-length sequence in the batch.\n",
    "        # Hence, the padded sequence length can be different for different batches.\n",
    "        batch_rgb, batch_labels, batch_lens = tf.train.batch_join(sample_list,\n",
    "                                                    batch_size=config['batch_size'],\n",
    "                                                    capacity=config['ip_queue_capacity'],\n",
    "                                                    enqueue_many=False,\n",
    "                                                    dynamic_pad=True,\n",
    "                                                    allow_smaller_final_batch = False,\n",
    "                                                    name=\"batch_join_and_pad\")\n",
    "\n",
    "        return batch_rgb, batch_labels, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel():\n",
    "    \"\"\"\n",
    "    Creates training and validation computational graphs.\n",
    "    Note that tf.variable_scope enables sharing the parameters so that both graphs share the parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, input_op, mode):\n",
    "        \"\"\"\n",
    "        Basic setup.\n",
    "        Args:\n",
    "          config: Object containing configuration parameters.\n",
    "        \"\"\"\n",
    "        assert mode in [\"training\", \"validation\"]\n",
    "        self.config = config\n",
    "        self.inputs = input_op\n",
    "        self.mode = mode\n",
    "        self.is_training = self.mode == \"training\"\n",
    "        self.reuse = self.mode == \"validation\"\n",
    "        \n",
    "        \n",
    "    def build_model(self, input_layer):\n",
    "        with tf.variable_scope(\"cnn_model\", reuse=self.reuse, initializer=self.config['initializer']):\n",
    "            # Convolutional Layer #1\n",
    "            # Computes 32 features using a 3x3 filter with ReLU activation.\n",
    "            # Padding is added to preserve width and height.\n",
    "            # Input Tensor Shape: [batch_size, 80, 80, num_channels]\n",
    "            # Output Tensor Shape: [batch_size, 40, 40, num_filter1]\n",
    "            conv1 = tf.layers.conv2d(\n",
    "                inputs=input_layer,\n",
    "                filters=self.config['cnn_filters'][0],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2, padding='same')\n",
    "\n",
    "            # Input Tensor Shape: [batch_size, 40, 40, num_filter1]\n",
    "            # Output Tensor Shape: [batch_size, 20, 20, num_filter2]\n",
    "            conv2 = tf.layers.conv2d(\n",
    "                inputs=pool1,\n",
    "                filters=self.config['cnn_filters'][1],\n",
    "                kernel_size=[5, 5],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2, padding='same')\n",
    "\n",
    "            # Input Tensor Shape: [batch_size, 20, 20, num_filter2]\n",
    "            # Output Tensor Shape: [batch_size, 10, 10, num_filter3]\n",
    "            conv3 = tf.layers.conv2d(\n",
    "                inputs=pool2,\n",
    "                filters=self.config['cnn_filters'][2],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2, padding='same')\n",
    "\n",
    "            # Input Tensor Shape: [batch_size, 10, 10, num_filter3]\n",
    "            # Output Tensor Shape: [batch_size, 5, 5, num_filter4]\n",
    "            conv4 = tf.layers.conv2d(\n",
    "                inputs=pool3,\n",
    "                filters=self.config['cnn_filters'][3],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "            pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2, padding='same')\n",
    "\n",
    "\n",
    "            # Flatten tensor into a batch of vectors\n",
    "            # Input Tensor Shape: [batch_size, 5, 5, num_filter4]\n",
    "            # Output Tensor Shape: [batch_size, 5 * 5 * num_filter4]\n",
    "            conv_flat = tf.reshape(pool4, [-1, 5 * 5 * self.config['cnn_filters'][3]])\n",
    "\n",
    "            # Add dropout operation;\n",
    "            dropout = tf.layers.dropout(inputs=conv_flat, rate=self.config['dropout_rate'], training=self.is_training)\n",
    "\n",
    "            # Dense Layer\n",
    "            # Densely connected layer with <num_hidden_units> neurons\n",
    "            # Input Tensor Shape: [batch_size, 5 * 5 * num_filter4]\n",
    "            # Output Tensor Shape: [batch_size, num_hidden_units]\n",
    "            dense = tf.layers.dense(inputs=dropout, units=self.config['num_hidden_units'],\n",
    "                activation=tf.nn.relu)\n",
    "        \n",
    "            self.cnn_model = dense\n",
    "            return dense\n",
    "            \n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        CNNs accept inputs of shape (batch_size, height, width, num_channels). However, we have inputs of shape\n",
    "        (batch_size, sequence_length, height, width, num_channels) where sequence_length is inferred at run time.\n",
    "        We need to iterate in order to get CNN representations. Similar to python's map function, \"tf.map_fn\" \n",
    "        applies a given function on each entry in the input list. \n",
    "        \"\"\"\n",
    "        # For the first time create a dummy graph and then share the parameters everytime.\n",
    "        if self.is_training:\n",
    "            self.reuse = False\n",
    "            self.build_model(self.inputs[0])\n",
    "            self.reuse = True\n",
    "        \n",
    "        # CNN takes a clip as if it is a batch of samples.\n",
    "        # Have a look at tf.map_fn (https://www.tensorflow.org/api_docs/python/tf/map_fn)\n",
    "        # You can set parallel_iterations or swap_memory in order to make it faster.\n",
    "        # Note that back_prop argument is True in order to enable training of CNN.\n",
    "        self.cnn_representations = tf.map_fn(lambda x: self.build_model(x),\n",
    "                                                elems=self.inputs,\n",
    "                                                dtype=tf.float32,\n",
    "                                                back_prop=True,\n",
    "                                                swap_memory=True,\n",
    "                                                parallel_iterations=2)\n",
    "        \n",
    "        return self.cnn_representations\n",
    "        \n",
    "\n",
    "class RNNModel():\n",
    "    \"\"\"\n",
    "    Creates training and validation computational graphs.\n",
    "    Note that tf.variable_scope enables sharing the parameters so that both graphs share the parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, input_op, target_op, seq_len_op, mode):\n",
    "        \"\"\"\n",
    "        Basic setup.\n",
    "        Args:\n",
    "          config: Object containing configuration parameters.\n",
    "        \"\"\"\n",
    "        assert mode in [\"training\", \"validation\"]\n",
    "        self.config = config\n",
    "        self.inputs = input_op\n",
    "        self.targets = target_op\n",
    "        self.seq_lengths = seq_len_op\n",
    "        self.mode = mode\n",
    "        self.reuse = self.mode == \"validation\"\n",
    "        \n",
    "    def build_rnn_model(self):\n",
    "        with tf.variable_scope('rnn_cell', reuse=self.reuse, initializer=self.config['initializer']):\n",
    "            rnn_cell = tf.contrib.rnn.BasicLSTMCell(num_units=self.config['num_hidden_units'])\n",
    "        with tf.variable_scope('rnn_stack', reuse=self.reuse, initializer=self.config['initializer']):\n",
    "            if self.config['num_layers'] > 1:\n",
    "                rnn_cell = tf.contrib.rnn.MultiRNNCell([rnn_cell for _ in range(self.config['num_layers'])])\n",
    "            self.model_rnn, self.rnn_state = tf.nn.dynamic_rnn(\n",
    "                                            cell=rnn_cell,\n",
    "                                            inputs=self.inputs,\n",
    "                                            dtype = tf.float32,\n",
    "                                            sequence_length=self.seq_lengths,\n",
    "                                            time_major=False,\n",
    "                                            swap_memory=True)\n",
    "            # Fetch output of the last step.\n",
    "            if self.config['loss_type'] == 'last_step':\n",
    "                self.rnn_prediction = tf.gather_nd(self.model_rnn, tf.stack([tf.range(self.config['batch_size']), self.seq_lengths-1], axis=1))\n",
    "            elif self.config['loss_type'] == 'average':\n",
    "                self.rnn_prediction = self.model_rnn\n",
    "            else:\n",
    "                print(\"Invalid loss type\")\n",
    "                raise\n",
    "                \n",
    "    \n",
    "    def build_model(self):\n",
    "        self.build_rnn_model()\n",
    "        # Calculate logits\n",
    "        with tf.variable_scope('logits', reuse=self.reuse, initializer=self.config['initializer']):\n",
    "            self.logits = tf.layers.dense(inputs=self.rnn_prediction, units=self.config['num_class_labels'],\n",
    "                                            kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                            bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            # In the case of average loss, take average of time steps in order to calculate\n",
    "            # final prediction probabilities.\n",
    "            if self.config['loss_type'] == 'average':\n",
    "                self.logits = tf.reduce_mean(self.logits, axis=1)\n",
    "            \n",
    "    def loss(self):\n",
    "        # Loss calculations: cross-entropy\n",
    "        with tf.name_scope(\"cross_entropy_loss\"):\n",
    "            self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.targets))\n",
    "            \n",
    "                # Accuracy calculations.\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # Return list of predictions (useful for making a submission)\n",
    "            self.predictions = tf.argmax(self.logits, 1, name=\"predictions\")\n",
    "            # Return a bool tensor with shape [batch_size] that is true for the\n",
    "            # correct predictions.\n",
    "            self.correct_predictions = tf.equal(tf.argmax(self.logits, 1), self.targets)\n",
    "            # Number of correct predictions in order to calculate average accuracy afterwards.\n",
    "            self.num_correct_predictions = tf.reduce_sum(tf.cast(self.correct_predictions, tf.int32))\n",
    "            # Calculate the accuracy per minibatch.\n",
    "            self.batch_accuracy = tf.reduce_mean(tf.cast(self.correct_predictions, tf.float32))\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self.build_model()\n",
    "        self.loss()\n",
    "        self.num_parameters()\n",
    "        \n",
    "        return self.logits, self.loss, self.batch_accuracy, self.predictions\n",
    "    \n",
    "    def num_parameters(self):\n",
    "        self.num_parameters = 0\n",
    "        #iterating over all variables\n",
    "        for variable in tf.trainable_variables():\n",
    "            local_parameters=1\n",
    "            shape = variable.get_shape()  #getting shape of a variable\n",
    "            for i in shape:\n",
    "                local_parameters*=i.value  #mutiplying dimension values\n",
    "            self.num_parameters+=local_parameters\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of tfRecord input files.\n",
    "train_filenames = [os.path.join(config['train_data_dir'], config['train_file_format'] % i) for i in config['train_file_ids']]\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "train_batch_samples_op, train_batch_labels_op, train_batch_seq_len_op = input_pipeline(train_filenames, config, name='training_input_pipeline')\n",
    "\n",
    "# Create a list of tfRecord input files.\n",
    "valid_filenames = [os.path.join(config['valid_data_dir'], config['valid_file_format'] % i) for i in config['valid_file_ids']]\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "valid_batch_samples_op, valid_batch_labels_op, valid_batch_seq_len_op = input_pipeline(valid_filenames, config, name='validation_input_pipeline', shuffle=False)\n",
    "\n",
    "# Create placeholders for training and monitoring variables.\n",
    "loss_avg_op = tf.placeholder(tf.float32, name=\"loss_avg\")\n",
    "accuracy_avg_op = tf.placeholder(tf.float32, name=\"accuracy_avg\")\n",
    "\n",
    "# Generate a variable to contain a counter for the global training step.\n",
    "# Note that it is useful if you save/restore your network.\n",
    "global_step = tf.Variable(1, name='global_step', trainable=False)\n",
    "\n",
    "# Create seperate graphs for training and validation.\n",
    "# Training graph\n",
    "# Note that our model is optimized by using the training graph.\n",
    "with tf.name_scope(\"Training\"):\n",
    "    # Create model\n",
    "    cnnModel = CNNModel(config=config['cnn'],\n",
    "                        input_op=train_batch_samples_op, \n",
    "                        mode='training')\n",
    "    cnn_representations = cnnModel.build_graph()\n",
    "    \n",
    "    trainModel = RNNModel(config=config['rnn'], \n",
    "                            input_op=cnn_representations, \n",
    "                            target_op=train_batch_labels_op, \n",
    "                            seq_len_op=train_batch_seq_len_op,\n",
    "                            mode=\"training\")\n",
    "    trainModel.build_graph()\n",
    "    print(\"\\n# of parameters: %s\" % trainModel.num_parameters)\n",
    "    \n",
    "    # Optimization routine.\n",
    "    # Learning rate is decayed in time. This enables our model using higher learning rates in the beginning.\n",
    "    # In time the learning rate is decayed so that gradients don't explode and training staurates.\n",
    "    # If you observe slow training, feel free to modify decay_steps and decay_rate arguments.\n",
    "    if config['learning_rate_type'] == 'exponential':\n",
    "        learning_rate = tf.train.exponential_decay(config['learning_rate'], \n",
    "                                                   global_step=global_step,\n",
    "                                                   decay_steps=1000, \n",
    "                                                   decay_rate=0.97,\n",
    "                                                   staircase=False)\n",
    "    elif config['learning_rate_type'] == 'fixed':\n",
    "        learning_rate = config['learning_rate']\n",
    "    else:\n",
    "        print(\"Invalid learning rate type\")\n",
    "        raise\n",
    "        \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(trainModel.loss, global_step=global_step)\n",
    "\n",
    "# Validation graph.\n",
    "with tf.name_scope(\"Evaluation\"):\n",
    "    # Create model\n",
    "    validCnnModel = CNNModel(config=config['cnn'],\n",
    "                                input_op=valid_batch_samples_op, \n",
    "                                mode='validation')\n",
    "    valid_cnn_representations = validCnnModel.build_graph()\n",
    "    \n",
    "    validModel = RNNModel(config=config['rnn'], \n",
    "                            input_op=valid_cnn_representations, \n",
    "                            target_op=valid_batch_labels_op, \n",
    "                            seq_len_op=valid_batch_seq_len_op,\n",
    "                            mode=\"validation\")\n",
    "    validModel.build_graph()\n",
    "\n",
    "    \n",
    "# Create summary ops for monitoring the training.\n",
    "# Each summary op annotates a node in the computational graph and collects\n",
    "# data data from it.\n",
    "summary_train_loss = tf.summary.scalar('loss', trainModel.loss)\n",
    "summary_train_acc = tf.summary.scalar('accuracy_training', trainModel.batch_accuracy)\n",
    "summary_avg_accuracy = tf.summary.scalar('accuracy_avg', accuracy_avg_op)\n",
    "summary_avg_loss = tf.summary.scalar('loss_avg', loss_avg_op)\n",
    "summary_learning_rate = tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "# Group summaries.\n",
    "# summaries_training is used during training and reported after every step.\n",
    "summaries_training = tf.summary.merge([summary_train_loss, summary_train_acc, summary_learning_rate])\n",
    "# summaries_evaluation is used by both trainig and validation in order to report the performance on the dataset.\n",
    "summaries_evaluation = tf.summary.merge([summary_avg_accuracy, summary_avg_loss])\n",
    "    \n",
    "#Create session object\n",
    "sess = tf.Session()\n",
    "# Add the ops to initialize variables.\n",
    "init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "# Actually intialize the variables\n",
    "sess.run(init_op)\n",
    "\n",
    "# Register summary ops.\n",
    "train_summary_dir = os.path.join(config['model_dir'], \"summary\", \"train\")\n",
    "train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "valid_summary_dir = os.path.join(config['model_dir'], \"summary\", \"validation\")\n",
    "valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "\n",
    "# Create a saver for writing training checkpoints.\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "# Define counters in order to accumulate measurements.\n",
    "counter_correct_predictions_training = 0.0\n",
    "counter_loss_training = 0.0\n",
    "counter_correct_predictions_validation = 0.0\n",
    "counter_loss_validation = 0.0\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "try:\n",
    "    while not coord.should_stop():\n",
    "        step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "        if (step%config['checkpoint_every_step']) == 0:\n",
    "            ckpt_save_path = saver.save(sess, os.path.join(config['model_dir'], 'model'), global_step)\n",
    "            print(\"Model saved in file: %s\" % ckpt_save_path)\n",
    "            \n",
    "        # Run the optimizer to update weights.\n",
    "        # Note that \"train_op\" is responsible from updating network weights.\n",
    "        # Only the operations that are fed are evaluated.\n",
    "        # Run the optimizer to update weights.\n",
    "        train_summary, num_correct_predictions, loss, _ = sess.run([summaries_training, \n",
    "                                                                      trainModel.num_correct_predictions, \n",
    "                                                                      trainModel.loss, \n",
    "                                                                      train_op], \n",
    "                                                                      feed_dict={})\n",
    "        # Update counters.\n",
    "        counter_correct_predictions_training += num_correct_predictions\n",
    "        counter_loss_training += loss\n",
    "        # Write summary data.\n",
    "        train_summary_writer.add_summary(train_summary, step)\n",
    "        \n",
    "        # Report training performance\n",
    "        if (step%config['print_every_step']) == 0:\n",
    "            accuracy_avg = counter_correct_predictions_training / (config['batch_size']*config['print_every_step'])\n",
    "            loss_avg = counter_loss_training / (config['print_every_step'])\n",
    "            summary_report = sess.run(summaries_evaluation, feed_dict={accuracy_avg_op:accuracy_avg, loss_avg_op:loss_avg})\n",
    "            train_summary_writer.add_summary(summary_report, step)\n",
    "            print(\"[%d/%d] [Training] Accuracy: %.3f, Loss: %.3f\" % (step/config['num_steps_per_epoch'], \n",
    "                                                                     step, \n",
    "                                                                     accuracy_avg, \n",
    "                                                                     loss_avg))\n",
    "            \n",
    "            counter_correct_predictions_training = 0.0\n",
    "            counter_loss_training= 0.0\n",
    "        \n",
    "        if (step%config['evaluate_every_step']) == 0:\n",
    "            # It is possible to create only one input pipelene queue. Hence, we create a validation queue \n",
    "            # in the begining for multiple epochs and control it via a foor loop.\n",
    "            # Note that we only approximate 1 validation epoch (validation doesn't have to be accurate.)\n",
    "            # In other words, number of unique validation samples may differ everytime.\n",
    "            for eval_step in range(config['num_validation_steps']):\n",
    "                # Calculate average validation accuracy.\n",
    "                num_correct_predictions, loss = sess.run([validModel.num_correct_predictions, \n",
    "                                                          validModel.loss],\n",
    "                                                         feed_dict={})\n",
    "                # Update counters.\n",
    "                counter_correct_predictions_validation += num_correct_predictions\n",
    "                counter_loss_validation += loss\n",
    "            \n",
    "            # Report validation performance\n",
    "            accuracy_avg = counter_correct_predictions_validation / (config['batch_size']*config['num_validation_steps'])\n",
    "            loss_avg = counter_loss_validation / (config['num_validation_steps'])\n",
    "            summary_report = sess.run(summaries_evaluation, feed_dict={accuracy_avg_op:accuracy_avg, loss_avg_op:loss_avg})\n",
    "            valid_summary_writer.add_summary(summary_report, step)\n",
    "            print(\"[%d/%d] [Validation] Accuracy: %.3f, Loss: %.3f\" % (step/config['num_steps_per_epoch'], \n",
    "                                                                       step, \n",
    "                                                                       accuracy_avg, \n",
    "                                                                       loss_avg))\n",
    "            \n",
    "            counter_correct_predictions_validation = 0.0\n",
    "            counter_loss_validation= 0.0\n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Model is trained for %d epochs, %d steps.' % (config['num_epochs'], step))\n",
    "    print('Done.')\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()\n",
    "\n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)\n",
    "\n",
    "ckpt_save_path = saver.save(sess, os.path.join(config['model_dir'], 'model'), global_step)\n",
    "print(\"Model saved in file: %s\" % ckpt_save_path)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A simple sanity check for input pipeline:\n",
    "'''\n",
    "def test_input_loader():\n",
    "    config = {}\n",
    "    config['img_height'] = 80\n",
    "    config['img_width'] = 80\n",
    "    config['img_num_channels'] = 3\n",
    "    config['num_epochs'] = 10\n",
    "    config['batch_size'] = 16\n",
    "    # Capacity of the queue which contains the samples read by data readers.\n",
    "    # Make sure that it has enough capacity.\n",
    "    config['ip_queue_capacity'] = config['batch_size']*10  \n",
    "    config['ip_num_read_threads'] = 6\n",
    "    # Directory of the data.\n",
    "    config['data_dir'] = \"/home/nico/git/uieproject/data/train\"\n",
    "    # File naming\n",
    "    config['file_format'] = \"dataTrain_%d.tfrecords\"\n",
    "    # File IDs to be used for training.\n",
    "    config['file_ids'] = list(range(1,10))\n",
    "    \n",
    "    # Create a list of TFRecord input files.\n",
    "    filenames = [os.path.join(config['data_dir'], config['file_format'] % i) for i in config['file_ids']]\n",
    "\n",
    "    # Create data loading operators. This will be represented as a node in the computational graph.\n",
    "    batch_samples_op, batch_labels_op, batch_seq_len_op = input_pipeline(filenames, config)\n",
    "    # TODO: batch_samples_op, batch_labels_op and batch_seq_len_op are like input placeholders. You can directly \n",
    "    # feed them to your model.\n",
    "\n",
    "    # Create tensorflow session and initialize the variables (if any).\n",
    "    sess = tf.Session()\n",
    "    init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    # Create threads to prefetch the data.\n",
    "    # https://www.tensorflow.org/programmers_guide/reading_data#creating_threads_to_prefetch_using_queuerunner_objects\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    batch_samples, batch_labels, batch_seq_len = sess.run([batch_samples_op, batch_labels_op, batch_seq_len_op])\n",
    "    \n",
    "    # Print \n",
    "    print(\"# Samples: \" + str(len(batch_samples)))\n",
    "    print(\"Sequence lengths: \" + str(batch_seq_len))\n",
    "    print(\"Sequence labels: \" + str(batch_labels))\n",
    "    \n",
    "    # Note that the second dimension will give maximum-length in the batch, i.e., the padded sequence length.\n",
    "    print(\"Sequence type: \" + str(type(batch_samples)))\n",
    "    print(\"Sequence shape: \" + str(batch_samples.shape))\n",
    "\n",
    "    # Fetch first clips 11th frame.\n",
    "    img = batch_samples[0][10]\n",
    "    print(\"(flattened) Image shape: \" + str(img.shape))\n",
    "    img = np.reshape(img, (32,32))\n",
    "    print(\"Image shape: \" + str(img.shape))\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img) # Note that image may look wierd because it is normalized.\n",
    "    \n",
    "test_input_loader()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
